{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khaled-Wael/NASA_Project/blob/main/Nasa%20code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXOPLANET DETECTION PIPELINE\n",
        "!pip install lightkurve\n",
        "!pip install tensorflow\n",
        "!pip install keras_tuner\n",
        "\n",
        "#Libraries Upload\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as pltn\n",
        "import seaborn as sns\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy import stats\n",
        "import lightkurve as lk\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, auc\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
        "import keras_tuner as kt\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"✅ Libraries imported successfully\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Lightkurve version: {lk.__version__}\")\n",
        "\n",
        "# Data Loading\n",
        "# 1. DATA LOADING AND PARSING\n",
        "# ============================================\n",
        "def load_and_parse_datasets():\n",
        "    \"\"\"Load and merge datasets for stellar params\"\"\"\n",
        "    datasets = {}\n",
        "\n",
        "    # Load K2\n",
        "    print(\"Loading K2 dataset...\")\n",
        "    with open('/content/sample_data/k2pandc_2025.09.25_19.49.42.csv', 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        header_line = None\n",
        "        for i, line in enumerate(lines):\n",
        "            if 'pl_name' in line and 'hostname' in line:\n",
        "                header_line = i\n",
        "                break\n",
        "        if header_line:\n",
        "            k2_data = pd.read_csv('/content/sample_data/k2pandc_2025.09.25_19.49.42.csv', skiprows=header_line, sep=',', engine='python')\n",
        "        else:\n",
        "            k2_data = pd.read_csv('/content/sample_data/k2pandc_2025.09.25_19.49.42.csv', skiprows=97, header=None, names=['raw_data'])\n",
        "    datasets['K2'] = k2_data\n",
        "\n",
        "    # Load ExoMiner\n",
        "    print(\"Loading ExoMiner dataset...\")\n",
        "    exominer_data = pd.read_csv('/content/sample_data/exominer_vetting_tess-spoc-2-min-s1s67_dashtable_dvm-url_scoregt0.1 (1).csv')\n",
        "    datasets['ExoMiner'] = exominer_data\n",
        "\n",
        "    # Load TOI\n",
        "    print(\"Loading TOI dataset...\")\n",
        "    try:\n",
        "        toi_data = pd.read_csv('/content/sample_data/TOI_2025.09.25_19.49.24.csv', comment='#', engine='python')\n",
        "        datasets['TOI'] = toi_data\n",
        "    except:\n",
        "        datasets['TOI'] = pd.DataFrame()\n",
        "\n",
        "    # Merge ExoMiner with TOI\n",
        "    if not datasets['TOI'].empty:\n",
        "        print(\"Merging ExoMiner with TOI...\")\n",
        "        exominer_data = exominer_data.merge(toi_data, left_on='TIC ID', right_on='tid', how='left')\n",
        "        exominer_data = exominer_data.rename(columns={\n",
        "            'st_teff': 'Stellar Effective Temperature [K]',\n",
        "            'st_rad': 'Stellar Radius [Solar Radii]',\n",
        "            'st_logg': 'Stellar Surface Gravity [log10(cm/s*2)]'\n",
        "        })\n",
        "        datasets['ExoMiner'] = exominer_data\n",
        "\n",
        "    # Load Cumulative\n",
        "    print(\"Loading Cumulative dataset...\")\n",
        "    try:\n",
        "        cumulative_data = pd.read_csv('/content/sample_data/cumulative_2025.09.25_19.49.05.csv', comment='#', engine='python')\n",
        "        datasets['Cumulative'] = cumulative_data\n",
        "    except:\n",
        "        datasets['Cumulative'] = pd.DataFrame()\n",
        "\n",
        "    return datasets\n",
        "\n",
        "# Data Preprocessing and Feature Engineering\n",
        "class ExoplanetDataPreprocessor:\n",
        "    \"\"\"Preprocess exoplanet data for machine learning\"\"\"\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()  # For all features\n",
        "        self.stellar_scaler = StandardScaler()  # For stellar features only\n",
        "        self.feature_columns = []\n",
        "        self.scaler_file = 'scaler.pkl'\n",
        "        self.stellar_scaler_file = 'stellar_scaler.pkl'\n",
        "\n",
        "    def create_labels(self, datasets):\n",
        "        \"\"\"Create binary labels for classification\"\"\"\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "        exominer_data['label'] = (exominer_data['ExoMiner Score'] > 0.7).astype(int)\n",
        "        print(\"✅ Labels created\")\n",
        "        print(f\"ExoMiner - Planets: {exominer_data['label'].sum()}, \"\n",
        "              f\"Non-planets: {len(exominer_data) - exominer_data['label'].sum()}\")\n",
        "        return datasets\n",
        "\n",
        "    def engineer_features(self, datasets):\n",
        "        \"\"\"Create features from available data\"\"\"\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "\n",
        "        # Define feature columns\n",
        "        transit_columns = [\n",
        "            'Orbital Period [day]',\n",
        "            'Transit Duration [hour]',\n",
        "            'Transit Depth [ppm]',\n",
        "            'Planet Radius [Earth Radii]',\n",
        "            'MES',\n",
        "            'Transit Model SNR',\n",
        "            'Number of transits observed'\n",
        "        ]\n",
        "        stellar_columns = [\n",
        "            'Stellar Effective Temperature [K]',\n",
        "            'Stellar Radius [Solar Radii]',\n",
        "            'Stellar Surface Gravity [log10(cm/s*2)]'\n",
        "        ]\n",
        "        feature_columns = transit_columns + stellar_columns\n",
        "        features = exominer_data[feature_columns].copy()\n",
        "\n",
        "        # Impute missing\n",
        "        features = features.fillna(features.mean())\n",
        "\n",
        "        # Store original index\n",
        "        original_index = features.index\n",
        "\n",
        "        # Remove outliers\n",
        "        features_cleaned = self.remove_outliers_iqr(features, threshold=2.0)\n",
        "\n",
        "        # Cleaned index\n",
        "        cleaned_index = features_cleaned.index\n",
        "\n",
        "        # Derived features\n",
        "        features_cleaned['depth_duration_ratio'] = features_cleaned['Transit Depth [ppm]'] / features_cleaned['Transit Duration [hour]']\n",
        "        features_cleaned['period_snr_ratio'] = features_cleaned['Orbital Period [day]'] / features_cleaned['Transit Model SNR']\n",
        "\n",
        "        # Scale stellar features separately\n",
        "        stellar_features = features_cleaned[stellar_columns]\n",
        "        stellar_scaled = self.stellar_scaler.fit_transform(stellar_features)\n",
        "        with open(self.stellar_scaler_file, 'wb') as f:\n",
        "            pickle.dump(self.stellar_scaler, f)\n",
        "\n",
        "        # Scale all features\n",
        "        feature_names = features_cleaned.columns.tolist()\n",
        "        features_scaled = self.scaler.fit_transform(features_cleaned)\n",
        "        with open(self.scaler_file, 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "\n",
        "        features_df = pd.DataFrame(features_scaled, columns=feature_names, index=cleaned_index)\n",
        "        self.feature_columns = feature_names\n",
        "\n",
        "        datasets['ExoMiner_Features'] = features_df\n",
        "        print(\"✅ Features engineered\")\n",
        "        print(f\"Original samples: {len(original_index)}, After cleaning: {len(cleaned_index)}\")\n",
        "        print(f\"Feature columns: {self.feature_columns}\")\n",
        "        return datasets, cleaned_index\n",
        "\n",
        "    def remove_outliers_iqr(self, df, threshold=2.0):\n",
        "        \"\"\"Remove outliers using Interquartile Range method\"\"\"\n",
        "        clean_df = df.copy()\n",
        "        for column in df.columns:\n",
        "            if df[column].dtype in ['float64', 'int64']:\n",
        "                Q1 = df[column].quantile(0.25)\n",
        "                Q3 = df[column].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - threshold * IQR\n",
        "                upper_bound = Q3 + threshold * IQR\n",
        "                mask = (clean_df[column] >= lower_bound) & (clean_df[column] <= upper_bound)\n",
        "                clean_df = clean_df[mask]\n",
        "        return clean_df\n",
        "\n",
        "## LIGHT CURVE PROCESSING\n",
        "class LightCurveProcessor:\n",
        "    \"\"\"Process light curves for AstroNet input with global/local views\"\"\"\n",
        "    def __init__(self, global_length=2001, local_length=201):\n",
        "        self.global_length = global_length\n",
        "        self.local_length = local_length\n",
        "\n",
        "    def download_single_lightcurve(self, tic_id):\n",
        "        \"\"\"Download TESS light curve for a given TIC ID (with caching)\"\"\"\n",
        "        cache_file = f\"lightcurve_{tic_id}.pkl\"\n",
        "        if os.path.exists(cache_file):\n",
        "            try:\n",
        "                return pd.read_pickle(cache_file)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Cache read error for TIC {tic_id}: {e}\")\n",
        "                os.remove(cache_file)\n",
        "\n",
        "        try:\n",
        "            search_result = lk.search_lightcurve(f\"TIC {tic_id}\", mission='TESS', author='SPOC')\n",
        "            if len(search_result) == 0:\n",
        "                print(f\"❌ No light curves found for TIC {tic_id}\")\n",
        "                return None\n",
        "\n",
        "            lc_collection = search_result.download_all()\n",
        "            if lc_collection is None or len(lc_collection) == 0:\n",
        "                print(f\"❌ No data downloaded for TIC {tic_id}\")\n",
        "                return None\n",
        "\n",
        "            lc = lc_collection.stitch().remove_nans().normalize()\n",
        "\n",
        "            time = np.array(lc.time.value, dtype=float)\n",
        "            flux = np.array(lc.flux.value, dtype=float)\n",
        "            flux_err = np.array(lc.flux_err.value, dtype=float) if lc.flux_err is not None else np.full_like(flux, np.nan)\n",
        "\n",
        "            lc_data = {'tic_id': tic_id, 'time': time, 'flux': flux, 'flux_err': flux_err}\n",
        "            pd.to_pickle(lc_data, cache_file)\n",
        "\n",
        "            print(f\"✅ Downloaded TIC {tic_id} with {len(time)} points\")\n",
        "            return lc_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Download error for TIC {tic_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def preprocess_lightcurve(self, time, flux, period=None, detrend=True, normalize=True, augment=False):\n",
        "        \"\"\"Preprocess single light curve\"\"\"\n",
        "        mask = np.isfinite(time) & np.isfinite(flux)\n",
        "        time_clean, flux_clean = time[mask], flux[mask]\n",
        "        if len(flux_clean) < 100:\n",
        "            return None, None\n",
        "\n",
        "        if normalize:\n",
        "            flux_clean = flux_clean / np.median(flux_clean)\n",
        "\n",
        "        if detrend and len(flux_clean) > 101:\n",
        "            try:\n",
        "                window_length = min(101, len(flux_clean) - 1)\n",
        "                if window_length % 2 == 0:\n",
        "                    window_length -= 1\n",
        "                trend = savgol_filter(flux_clean, window_length, 2)\n",
        "                flux_clean /= trend\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Detrending failed: {e}\")\n",
        "\n",
        "        if augment:\n",
        "            flux_clean += np.random.normal(0, 0.01 * np.std(flux_clean), len(flux_clean))\n",
        "\n",
        "        # Create global view (full light curve)\n",
        "        global_view = self.resample_to_fixed_length(flux_clean, self.global_length)\n",
        "\n",
        "        # Create local view (zoomed around center or mid transit)\n",
        "        mid_idx = len(flux_clean) // 2\n",
        "        half_window = self.local_length // 2\n",
        "        start = max(0, mid_idx - half_window)\n",
        "        end = min(len(flux_clean), mid_idx + half_window)\n",
        "        local_flux = flux_clean[start:end]\n",
        "        local_view = self.resample_to_fixed_length(local_flux, self.local_length)\n",
        "\n",
        "        return global_view, local_view\n",
        "\n",
        "    def resample_to_fixed_length(self, flux, length):\n",
        "        \"\"\"Resample flux to a fixed sequence length\"\"\"\n",
        "        if len(flux) == length:\n",
        "            return flux\n",
        "        x_old = np.linspace(0, 1, len(flux))\n",
        "        x_new = np.linspace(0, 1, length)\n",
        "        interp = interp1d(x_old, flux, kind='linear', fill_value='extrapolate')\n",
        "        return interp(x_new)\n",
        "def preprocess_lightcurve(self, time, flux, period=None, detrend=True, normalize=True, augment=False):\n",
        "    \"\"\"Preprocess single light curve\"\"\"\n",
        "    mask = np.isfinite(time) & np.isfinite(flux)\n",
        "    time_clean, flux_clean = time[mask], flux[mask]\n",
        "    if len(flux_clean) < 100:\n",
        "        return None, None\n",
        "\n",
        "    if normalize:\n",
        "        flux_clean = flux_clean / np.median(flux_clean)\n",
        "\n",
        "    if detrend and len(flux_clean) > 101:\n",
        "        try:\n",
        "            window_length = min(101, len(flux_clean) - 1)\n",
        "            if window_length % 2 == 0:\n",
        "                window_length -= 1\n",
        "            trend = savgol_filter(flux_clean, window_length, 2)\n",
        "            flux_clean /= trend\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Detrending failed: {e}\")\n",
        "\n",
        "    if augment:\n",
        "        flux_clean += np.random.normal(0, 0.01 * np.std(flux_clean), len(flux_clean))\n",
        "\n",
        "    # Create global view (full light curve)\n",
        "    global_view = self.resample_to_fixed_length(flux_clean, self.global_length)\n",
        "\n",
        "    # Create local view (zoomed around center or mid transit)\n",
        "    mid_idx = len(flux_clean) // 2\n",
        "    half_window = self.local_length // 2\n",
        "    start = max(0, mid_idx - half_window)\n",
        "    end = min(len(flux_clean), mid_idx + half_window)\n",
        "    local_flux = flux_clean[start:end]\n",
        "    local_view = self.resample_to_fixed_length(local_flux, self.local_length)\n",
        "\n",
        "    return global_view, local_view\n",
        "    def resample_to_fixed_length(self, flux, length):\n",
        "        \"\"\"Resample flux to a fixed sequence length\"\"\"\n",
        "        if len(flux) == length:\n",
        "            return flux\n",
        "        x_old = np.linspace(0, 1, len(flux))\n",
        "        x_new = np.linspace(0, 1, length)\n",
        "        interp = interp1d(x_old, flux, kind='linear', fill_value='extrapolate')\n",
        "        return interp(x_new)\n",
        "\n",
        "#Transfer Learning\n",
        "class ExoplanetTransferLearningModel:\n",
        "    \"\"\"Improved AstroNet-like model\"\"\"\n",
        "    def __init__(self, global_shape=(2001, 1), local_shape=(201, 1), stellar_shape=(3,), num_classes=1):\n",
        "        self.global_shape = global_shape\n",
        "        self.local_shape = local_shape\n",
        "        self.stellar_shape = stellar_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = None\n",
        "\n",
        "    def create_model(self):\n",
        "        global_input = layers.Input(shape=self.global_shape, name='global_lc')\n",
        "        local_input = layers.Input(shape=self.local_shape, name='local_lc')\n",
        "        stellar_input = layers.Input(shape=self.stellar_shape, name='stellar_params')\n",
        "\n",
        "        def cnn_branch(input_layer, filters=[16, 32, 64, 128, 256]):\n",
        "            x = input_layer\n",
        "            for f in filters:\n",
        "                x = layers.Conv1D(f, 5, activation='relu', padding='same')(x)\n",
        "                x = layers.MaxPooling1D(5, strides=2, padding='same')(x)\n",
        "            x = layers.Flatten()(x)\n",
        "            return x\n",
        "\n",
        "        x_global = cnn_branch(global_input)\n",
        "        x_local = cnn_branch(local_input)\n",
        "\n",
        "        x = layers.Concatenate()([x_global, x_local])\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        residual = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Add()([x, residual])\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "\n",
        "        s = layers.Dense(32, activation='relu')(stellar_input)\n",
        "        x = layers.Concatenate()([x, s])\n",
        "        x = layers.Dense(128, activation='relu')(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "\n",
        "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "        self.model = keras.Model(inputs=[global_input, local_input, stellar_input], outputs=outputs)\n",
        "        return self.model\n",
        "\n",
        "    def compile_model(self, learning_rate=0.001):\n",
        "        self.model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate),\n",
        "            loss=BinaryFocalCrossentropy(alpha=0.25, gamma=2.0),\n",
        "            metrics=['accuracy', keras.metrics.Precision(name='precision'),\n",
        "                     keras.metrics.Recall(name='recall'), keras.metrics.AUC(name='auc')]\n",
        "        )\n",
        "        print(\"✅ Model compiled with focal loss\")\n",
        "\n",
        "    def setup_callbacks(self):\n",
        "        return [\n",
        "            callbacks.EarlyStopping(monitor='val_auc', patience=20, mode='max', restore_best_weights=True),\n",
        "            callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=10, min_lr=1e-6, mode='max'),\n",
        "            callbacks.ModelCheckpoint('best_model.h5', monitor='val_auc', save_best_only=True, mode='max')\n",
        "        ]\n",
        "\n",
        "#Model Training and Evaluation\n",
        "class ModelTrainer:\n",
        "    \"\"\"Train and evaluate the exoplanet detection model\"\"\"\n",
        "    def __init__(self):\n",
        "        self.history = None\n",
        "\n",
        "    def prepare_lc_datasets(self, datasets, cleaned_index, processor, preprocessor):\n",
        "        global_lc = []\n",
        "        local_lc = []\n",
        "        stellar = []\n",
        "        labels = []\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "\n",
        "        for idx in cleaned_index:\n",
        "            tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "            period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "            label = exominer_data.loc[idx, 'label']\n",
        "            lc_data = processor.download_single_lightcurve(tic_id)\n",
        "            if lc_data is not None:\n",
        "                g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "                if g_flux is not None and l_flux is not None:\n",
        "                    global_lc.append(g_flux)\n",
        "                    local_lc.append(l_flux)\n",
        "                    stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]',\n",
        "                                                         'Stellar Radius [Solar Radii]',\n",
        "                                                         'Stellar Surface Gravity [log10(cm/s*2)]']].values\n",
        "                    stellar_row = np.nan_to_num(stellar_row)\n",
        "\n",
        "                    # Scale stellar features using the stellar scaler\n",
        "                    try:\n",
        "                        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "                            scaler = pickle.load(f)\n",
        "                        stellar_row = scaler.transform(stellar_row.reshape(1, -1)).flatten()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading stellar scaler for TIC {tic_id}: {e}. Using raw stellar features.\")\n",
        "                        stellar_row = stellar_row  # Fallback to raw features\n",
        "                    stellar.append(stellar_row)\n",
        "                    labels.append(label)\n",
        "\n",
        "        if not global_lc:\n",
        "            raise ValueError(\"No valid light curves processed.\")\n",
        "\n",
        "        global_lc = np.array(global_lc).reshape(-1, processor.global_length, 1)\n",
        "        local_lc = np.array(local_lc).reshape(-1, processor.local_length, 1)\n",
        "        stellar = np.array(stellar)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        X_train_g, X_test_g, X_train_l, X_test_l, X_train_s, X_test_s, y_train, y_test = train_test_split(\n",
        "            global_lc, local_lc, stellar, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "        )\n",
        "        X_train_g, X_val_g, X_train_l, X_val_l, X_train_s, X_val_s, y_train, y_val = train_test_split(\n",
        "            X_train_g, X_train_l, X_train_s, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        "        )\n",
        "\n",
        "        return X_train_g, X_val_g, X_test_g, X_train_l, X_val_l, X_test_l, X_train_s, X_val_s, X_test_s, y_train, y_val, y_test\n",
        "\n",
        "    def compute_class_weights(self, y_train):\n",
        "        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "        weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "        print(f\"📊 Class weights: {weight_dict}\")\n",
        "        return weight_dict\n",
        "\n",
        "    def train_model(self, model, X_train_g, X_train_l, X_train_s, y_train, X_val_g, X_val_l, X_val_s, y_val,\n",
        "                    epochs=200, batch_size=64, class_weight=None):\n",
        "        callbacks = model.setup_callbacks()\n",
        "        self.history = model.model.fit(\n",
        "            [X_train_g, X_train_l, X_train_s], y_train,\n",
        "            validation_data=([X_val_g, X_val_l, X_val_s], y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            class_weight=class_weight,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "        return self.history\n",
        "\n",
        "    def evaluate_model(self, model, X_test_g, X_test_l, X_test_s, y_test):\n",
        "        model.model = keras.models.load_model('best_model.h5')\n",
        "        y_pred_proba = model.model.predict([X_test_g, X_test_l, X_test_s], verbose=0)\n",
        "        y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "        test_accuracy = accuracy_score(y_test, y_pred)\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        print(f\"\\n📈 TEST RESULTS:\")\n",
        "        print(f\" Accuracy: {test_accuracy:.4f}\")\n",
        "        print(f\" ROC AUC: {roc_auc:.4f}\")\n",
        "        print(f\" Precision: {report['1']['precision']:.4f}\")\n",
        "        print(f\" Recall: {report['1']['recall']:.4f}\")\n",
        "        return y_pred, y_pred_proba\n",
        "\n",
        "    def cross_validate(self, model_class, datasets, cleaned_index, processor, preprocessor, n_splits=5):\n",
        "        auc_scores = []\n",
        "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "        labels = datasets['ExoMiner']['label'].loc[cleaned_index]\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
        "            print(f\"Cross-validation Fold {fold+1}/{n_splits}\")\n",
        "            train_cleaned = cleaned_index[train_idx]\n",
        "            val_cleaned = cleaned_index[val_idx]\n",
        "            X_train_g, X_val_g, X_train_l, X_val_l, X_train_s, X_val_s, y_train, y_val = \\\n",
        "                self.prepare_lc_datasets_for_cv(datasets, train_cleaned, val_cleaned, processor, preprocessor)\n",
        "\n",
        "            model = model_class()\n",
        "            model.create_model()\n",
        "            model.compile_model()\n",
        "            self.train_model(model, X_train_g, X_train_l, X_train_s, y_train,\n",
        "                           X_val_g, X_val_l, X_val_s, y_val, epochs=50)\n",
        "            y_pred_proba = model.model.predict([X_val_g, X_val_l, X_val_s], verbose=0)\n",
        "            fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
        "            auc_scores.append(auc(fpr, tpr))\n",
        "\n",
        "        print(f\"\\nCross-validation AUC: {np.mean(auc_scores):.3f} ± {np.std(auc_scores):.3f}\")\n",
        "        return auc_scores\n",
        "\n",
        "    def prepare_lc_datasets_for_cv(self, datasets, train_index, val_index, processor, preprocessor):\n",
        "        global_lc_train = []\n",
        "        local_lc_train = []\n",
        "        stellar_train = []\n",
        "        labels_train = []\n",
        "        global_lc_val = []\n",
        "        local_lc_val = []\n",
        "        stellar_val = []\n",
        "        labels_val = []\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "\n",
        "        # Process training data\n",
        "        for idx in train_index:\n",
        "            tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "            period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "            label = exominer_data.loc[idx, 'label']\n",
        "            lc_data = processor.download_single_lightcurve(tic_id)\n",
        "            if lc_data is not None:\n",
        "                g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "                if g_flux is not None and l_flux is not None:\n",
        "                    global_lc_train.append(g_flux)\n",
        "                    local_lc_train.append(l_flux)\n",
        "                    stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]',\n",
        "                                                         'Stellar Radius [Solar Radii]',\n",
        "                                                         'Stellar Surface Gravity [log10(cm/s*2)]']].values\n",
        "                    stellar_row = np.nan_to_num(stellar_row)\n",
        "                    try:\n",
        "                        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "                            scaler = pickle.load(f)\n",
        "                        stellar_row = scaler.transform(stellar_row.reshape(1, -1)).flatten()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading stellar scaler for TIC {tic_id}: {e}. Using raw stellar features.\")\n",
        "                        stellar_row = stellar_row\n",
        "                    stellar_train.append(stellar_row)\n",
        "                    labels_train.append(label)\n",
        "\n",
        "        # Process validation data\n",
        "        for idx in val_index:\n",
        "            tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "            period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "            label = exominer_data.loc[idx, 'label']\n",
        "            lc_data = processor.download_single_lightcurve(tic_id)\n",
        "            if lc_data is not None:\n",
        "                g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "                if g_flux is not None and l_flux is not None:\n",
        "                    global_lc_val.append(g_flux)\n",
        "                    local_lc_val.append(l_flux)\n",
        "                    stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]',\n",
        "                                                         'Stellar Radius [Solar Radii]',\n",
        "                                                         'Stellar Surface Gravity [log10(cm/s*2)]']].values\n",
        "                    stellar_row = np.nan_to_num(stellar_row)\n",
        "                    try:\n",
        "                        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "                            scaler = pickle.load(f)\n",
        "                        stellar_row = scaler.transform(stellar_row.reshape(1, -1)).flatten()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading stellar scaler for TIC {tic_id}: {e}. Using raw stellar features.\")\n",
        "                        stellar_row = stellar_row\n",
        "                    stellar_val.append(stellar_row)\n",
        "                    labels_val.append(label)\n",
        "\n",
        "        if not global_lc_train or not global_lc_val:\n",
        "            raise ValueError(\"No valid light curves processed for cross-validation.\")\n",
        "\n",
        "        X_train_g = np.array(global_lc_train).reshape(-1, processor.global_length, 1)\n",
        "        X_train_l = np.array(local_lc_train).reshape(-1, processor.local_length, 1)\n",
        "        X_train_s = np.array(stellar_train)\n",
        "        y_train = np.array(labels_train)\n",
        "        X_val_g = np.array(global_lc_val).reshape(-1, processor.global_length, 1)\n",
        "        X_val_l = np.array(local_lc_val).reshape(-1, processor.local_length, 1)\n",
        "        X_val_s = np.array(stellar_val)\n",
        "        y_val = np.array(labels_val)\n",
        "\n",
        "        return X_train_g, X_val_g, X_train_l, X_val_l, X_train_s, X_val_s, y_train, y_val\n",
        "\n",
        "#Hyper Parameter Tuning\n",
        "def tune_hyperparameters(X_train_g, X_train_l, X_train_s, y_train, X_val_g, X_val_l, X_val_s, y_val):\n",
        "    \"\"\"Tune hyperparameters using Keras Tuner\"\"\"\n",
        "    def build_model(hp):\n",
        "        model = ExoplanetTransferLearningModel()\n",
        "        model.create_model()\n",
        "        model.compile_model(learning_rate=hp.Float('lr', 1e-5, 1e-3, sampling='log'))\n",
        "        return model.model\n",
        "\n",
        "    tuner = kt.Hyperband(\n",
        "        build_model,\n",
        "        objective='val_auc',\n",
        "        max_epochs=200,\n",
        "        factor=3,\n",
        "        directory='tuner_dir',\n",
        "        project_name='exoplanet'\n",
        "    )\n",
        "\n",
        "    tuner.search([X_train_g, X_train_l, X_train_s], y_train,\n",
        "                 validation_data=([X_val_g, X_val_l, X_val_s], y_val),\n",
        "                 epochs=50)\n",
        "\n",
        "    best_hps = tuner.get_best_hyperparameters()[0]\n",
        "    print(f\"Best learning rate: {best_hps.get('lr')}\")\n",
        "    return tuner.get_best_models(1)[0], best_hps\n",
        "\n",
        "# ============================================\n",
        "# 7. VISUALIZATION AND ANALYSIS\n",
        "# ============================================\n",
        "def visualize_results(history, y_test, y_pred, y_pred_proba):\n",
        "    \"\"\"Visualize training history and evaluation metrics\"\"\"\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history.history['accuracy'], label='accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "    plt.plot(history.history['auc'], label='auc')\n",
        "    plt.plot(history.history['val_auc'], label='val_auc')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Metric Value')\n",
        "    plt.title('Training History')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot ROC curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=['Not Exoplanet', 'Exoplanet'],\n",
        "                yticklabels=['Not Exoplanet', 'Exoplanet'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "def test_with_real_lightcurves(model, datasets, preprocessor, num_samples=10):\n",
        "    \"\"\"Test the model with real light curves not used in training\"\"\"\n",
        "    print(f\"\\n🔬 Testing with {num_samples} real light curves:\")\n",
        "    exominer_data = datasets['ExoMiner']\n",
        "    all_indices = exominer_data.index\n",
        "    cleaned_index = datasets['ExoMiner_Features'].index\n",
        "    unseen_indices = all_indices.difference(cleaned_index)\n",
        "\n",
        "    if len(unseen_indices) < num_samples:\n",
        "        print(\"Not enough unseen samples to test.\")\n",
        "        return\n",
        "\n",
        "    test_indices = np.random.choice(unseen_indices, num_samples, replace=False)\n",
        "    processor = LightCurveProcessor()\n",
        "\n",
        "    try:\n",
        "        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "            scaler = pickle.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading stellar scaler: {e}. Using raw stellar features.\")\n",
        "        scaler = None\n",
        "\n",
        "    for idx in test_indices:\n",
        "        tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "        period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "        true_label = exominer_data.loc[idx, 'label']\n",
        "        lc_data = processor.download_single_lightcurve(tic_id)\n",
        "        if lc_data is not None:\n",
        "            g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "            if g_flux is not None and l_flux is not None:\n",
        "                stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]',\n",
        "                                                    'Stellar Radius [Solar Radii]',\n",
        "                                                    'Stellar Surface Gravity [log10(cm/s*2)]']].values\n",
        "                stellar_row = np.nan_to_num(stellar_row).reshape(1, -1)\n",
        "                if scaler is not None:\n",
        "                    stellar_row = scaler.transform(stellar_row).flatten()\n",
        "                g_flux = np.array(g_flux).reshape(1, processor.global_length, 1)\n",
        "                l_flux = np.array(l_flux).reshape(1, processor.local_length, 1)\n",
        "                y_pred_proba = model.model.predict([g_flux, l_flux, stellar_row], verbose=0)\n",
        "                predicted_label = (y_pred_proba > 0.5).astype(int).flatten()[0]\n",
        "                print(f\" TIC ID: {tic_id}, True Label: {true_label}, \"\n",
        "                      f\"Predicted Probability: {y_pred_proba[0][0]:.4f}, Predicted Label: {predicted_label}\")\n",
        "            else:\n",
        "                print(f\" TIC ID: {tic_id} - Preprocessing failed.\")\n",
        "        else:\n",
        "            print(f\" TIC ID: {tic_id} - Light curve download failed.\")\n",
        "\n",
        "def analyze_feature_importance(datasets):\n",
        "    \"\"\"Analyze feature importance using RandomForest\"\"\"\n",
        "    print(\"\\n📊 Analyzing Feature Importance:\")\n",
        "    exominer_features = datasets['ExoMiner_Features']\n",
        "    labels = datasets['ExoMiner'].loc[exominer_features.index, 'label']\n",
        "\n",
        "    if len(exominer_features) == 0 or len(labels) == 0:\n",
        "        print(\"Not enough data to analyze feature importance.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            exominer_features, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "        )\n",
        "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        rf_model.fit(X_train, y_train)\n",
        "        importances = rf_model.feature_importances_\n",
        "        feature_names = exominer_features.columns\n",
        "        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "        feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
        "        print(\"Top 10 Feature Importance:\")\n",
        "        print(feature_importance_df.head(10))\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x='importance', y='feature', data=feature_importance_df.head(10), palette='viridis')\n",
        "        plt.title('Top 10 Feature Importance from RandomForest')\n",
        "        plt.xlabel('Importance')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during feature importance analysis: {e}\")\n",
        "\n",
        "#Execution of pipeline\n",
        "def main():\n",
        "    \"\"\"Execute the exoplanet detection pipeline with a limit of 50 light curves\"\"\"\n",
        "    datasets = load_and_parse_datasets()\n",
        "    preprocessor = ExoplanetDataPreprocessor()\n",
        "    datasets = preprocessor.create_labels(datasets)\n",
        "    datasets, cleaned_index = preprocessor.engineer_features(datasets)\n",
        "\n",
        "    # Limit cleaned_index to 40 TIC IDs to stay within 50 total light curves\n",
        "    if len(cleaned_index) > 40:\n",
        "        cleaned_index = np.random.choice(cleaned_index, size=40, replace=False)\n",
        "        print(f\"✅ Limited cleaned_index to 40 TIC IDs for light curve downloads\")\n",
        "\n",
        "    processor = LightCurveProcessor()\n",
        "    trainer = ModelTrainer()\n",
        "\n",
        "    X_train_g, X_val_g, X_test_g, X_train_l, X_val_l, X_test_l, X_train_s, X_val_s, X_test_s, y_train, y_val, y_test = \\\n",
        "        trainer.prepare_lc_datasets(datasets, cleaned_index, processor, preprocessor)\n",
        "\n",
        "    best_model_hp, best_hps = tune_hyperparameters(\n",
        "        X_train_g, X_train_l, X_train_s, y_train, X_val_g, X_val_l, X_val_s, y_val\n",
        "    )\n",
        "\n",
        "    # Re-create and train the final model\n",
        "    model = ExoplanetTransferLearningModel()\n",
        "    model.create_model()\n",
        "    model.compile_model(learning_rate=best_hps.get('lr'))\n",
        "\n",
        "    X_train_g_full = np.concatenate((X_train_g, X_val_g))\n",
        "    X_train_l_full = np.concatenate((X_train_l, X_val_l))\n",
        "    X_train_s_full = np.concatenate((X_train_s, X_val_s))\n",
        "    y_train_full = np.concatenate((y_train, y_val))\n",
        "\n",
        "    class_weights = trainer.compute_class_weights(y_train_full)\n",
        "    history = trainer.train_model(\n",
        "        model, X_train_g_full, X_train_l_full, X_train_s_full, y_train_full,\n",
        "        X_test_g, X_test_l, X_test_s, y_test, epochs=200, batch_size=64, class_weight=class_weights\n",
        "    )\n",
        "\n",
        "    y_pred, y_pred_proba = trainer.evaluate_model(model, X_test_g, X_test_l, X_test_s, y_test)\n",
        "\n",
        "    visualize_results(history, y_test, y_pred, y_pred_proba)\n",
        "    test_with_real_lightcurves(model, datasets, preprocessor, num_samples=10)  # 10 test light curves\n",
        "    analyze_feature_importance(datasets)\n",
        "    auc_scores = trainer.cross_validate(ExoplanetTransferLearningModel, datasets, cleaned_index, processor, preprocessor)\n",
        "\n",
        "    print(f\"\\n🎉 PIPELINE COMPLETED! Total light curves downloaded: ~50 (40 from cleaned_index + 10 test)\")\n",
        "    return model, history\n",
        "# Run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        model, history = main()\n",
        "        print(\"\\n🎉 PIPELINE COMPLETED!\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9ZXT3diQrT2",
        "outputId": "a09266f6-e3df-4734-a0cd-204a780a90d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 73 Complete [00h 00m 18s]\n",
            "val_auc: 0.5\n",
            "\n",
            "Best val_auc So Far: 0.5\n",
            "Total elapsed time: 00h 19m 22s\n",
            "\n",
            "Search: Running Trial #74\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "1.1946e-05        |0.00070477        |lr\n",
            "3                 |3                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "4                 |4                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n",
            "✅ Model compiled with focal loss\n",
            "Epoch 1/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OpDYpyAtSYeO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}