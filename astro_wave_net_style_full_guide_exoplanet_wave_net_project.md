Got it âœ… You want to take the Google Research exoplanet-ml model (from their main repo, not just astrowavenet) and apply transfer learning on more/new data. Letâ€™s break this down step by step so you know exactly how to do it:

1. Identify the base model

The exoplanet-ml repo
 includes:

Astronet â†’ a deep CNN for light curve classification.

AstroWaveNet â†’ a WaveNet-like variant (time-series conv).

Both are TensorFlow 1.x implementations (they use tf.compat.v1). So if you want transfer learning, youâ€™ll either:

Reuse their saved checkpoints and fine-tune, or

Re-implement the model in PyTorch/TensorFlow 2 and load pretrained weights if available.

2. Transfer learning strategy

You have two main approaches:

A. Fine-tuning (classical TL)

Load their pretrained model (Astronet/AstroWaveNet).

Replace the final classifier layer (Dense -> 1 unit + sigmoid) with a new one that matches your datasetâ€™s classes (binary, or more if you extend to multi-class).

Freeze early layers (conv feature extractors).

Train only the final layers first â†’ then gradually unfreeze more layers.

B. Feature extractor approach

Use the model as a fixed feature extractor: run your light curves through the pretrained conv stack.

Save embeddings (latent features).

Train a new lightweight classifier (MLP, logistic regression, or even tree-based model) on those embeddings.

3. Practical steps (TensorFlow version)

If you stick with their TF1.x code:

import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

# Load graph + checkpoint
saver = tf.train.import_meta_graph("astronet/model.ckpt.meta")
sess = tf.Session()
saver.restore(sess, "astronet/model.ckpt")

# Get feature layer (before final dense)
feature_tensor = sess.graph.get_tensor_by_name("Astronet/final_hidden:0")

# Replace last layer
new_output = tf.layers.dense(feature_tensor, 1, activation=tf.nn.sigmoid, name="new_output")
loss = tf.losses.log_loss(labels, new_output)


Then re-train on your new dataset with a smaller learning rate (e.g. 1e-4).

4. Practical steps (PyTorch re-implementation â€” recommended)

Since TF1 is legacy, itâ€™s easier to:

Re-implement Astronet/AstroWaveNet in PyTorch (we already drafted a WaveNet-style model in the guide).

Initialize with pretrained weights if you can export them (TFâ†’PyTorch conversion, or train from scratch on their dataset).

For transfer learning:

model = WaveLikeNet()
# Freeze all layers
for param in model.parameters():
    param.requires_grad = False

# Replace head
model.fc = nn.Linear(model.fc.in_features, num_classes)  # 1 for binary, else adjust


First train only the new head.

Then unfreeze last few residual blocks and fine-tune with lower LR.

5. Expanding to more data

You can apply TL to:

TESS light curves (lots more than Kepler, but noisier).

Synthetic exoplanet injection datasets (inject fake transits into real stellar variability).

Multi-label problems (e.g., classifying types of variability along with exoplanet detection).

6. Next step I can do for you

I can:

Show how to load the Astronet TF checkpoint and fine-tune (TensorFlow 1.x).

Write a PyTorch re-implementation with transfer learning scaffolding.

Make a Colab notebook that demonstrates transfer learning using synthetic + real Kepler/TESS light curves.

ðŸ‘‰ Which path do you prefer? (stick with TensorFlow 1.x original code or move to a PyTorch TL implementation)
