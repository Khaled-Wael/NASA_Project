{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khaled-Wael/NASA_Project/blob/YU/Nasa_Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXOPLANET DETECTION PIPELINE\n",
        "!pip install lightkurve\n",
        "!pip install tensorflow\n",
        "!pip install keras_tuner\n",
        "\n",
        "#Libraries Upload\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as pltn\n",
        "import seaborn as sns\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy import stats\n",
        "import lightkurve as lk\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, auc\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
        "import keras_tuner as kt\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"âœ… Libraries imported successfully\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Lightkurve version: {lk.__version__}\")\n",
        "\n",
        "# Data Loading\n",
        "# 1. DATA LOADING AND PARSING\n",
        "# ============================================\n",
        "def load_and_parse_datasets():\n",
        "    \"\"\"Load and merge datasets for stellar params\"\"\"\n",
        "    datasets = {}\n",
        "\n",
        "    # Load K2\n",
        "    print(\"Loading K2 dataset...\")\n",
        "    with open('/content/sample_data/k2pandc_2025.09.25_19.49.42.csv', 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        header_line = None\n",
        "        for i, line in enumerate(lines):\n",
        "            if 'pl_name' in line and 'hostname' in line:\n",
        "                header_line = i\n",
        "                break\n",
        "        if header_line:\n",
        "            k2_data = pd.read_csv('/content/sample_data/k2pandc_2025.09.25_19.49.42.csv', skiprows=header_line, sep=',', engine='python')\n",
        "        else:\n",
        "            k2_data = pd.read_csv('/content/sample_data/k2pandc_2025.09.25_19.49.42.csv', skiprows=97, header=None, names=['raw_data'])\n",
        "    datasets['K2'] = k2_data\n",
        "\n",
        "    # Load ExoMiner\n",
        "    print(\"Loading ExoMiner dataset...\")\n",
        "    exominer_data = pd.read_csv('/content/sample_data/exominer_vetting_tess-spoc-2-min-s1s67_dashtable_dvm-url_scoregt0.1 (1).csv')\n",
        "    datasets['ExoMiner'] = exominer_data\n",
        "\n",
        "    # Load TOI\n",
        "    print(\"Loading TOI dataset...\")\n",
        "    try:\n",
        "        toi_data = pd.read_csv('/content/sample_data/TOI_2025.09.25_19.49.24.csv', comment='#', engine='python')\n",
        "        datasets['TOI'] = toi_data\n",
        "    except:\n",
        "        datasets['TOI'] = pd.DataFrame()\n",
        "\n",
        "    # Merge ExoMiner with TOI\n",
        "    if not datasets['TOI'].empty:\n",
        "        print(\"Merging ExoMiner with TOI...\")\n",
        "        exominer_data = exominer_data.merge(toi_data, left_on='TIC ID', right_on='tid', how='left')\n",
        "        exominer_data = exominer_data.rename(columns={\n",
        "            'st_teff': 'Stellar Effective Temperature [K]',\n",
        "            'st_rad': 'Stellar Radius [Solar Radii]',\n",
        "            'st_logg': 'Stellar Surface Gravity [log10(cm/s*2)]'\n",
        "        })\n",
        "        datasets['ExoMiner'] = exominer_data\n",
        "\n",
        "    # Load Cumulative\n",
        "    print(\"Loading Cumulative dataset...\")\n",
        "    try:\n",
        "        cumulative_data = pd.read_csv('/content/sample_data/cumulative_2025.09.25_19.49.05.csv', comment='#', engine='python')\n",
        "        datasets['Cumulative'] = cumulative_data\n",
        "    except:\n",
        "        datasets['Cumulative'] = pd.DataFrame()\n",
        "\n",
        "    return datasets\n",
        "\n",
        "# Data Preprocessing and Feature Engineering\n",
        "class ExoplanetDataPreprocessor:\n",
        "    \"\"\"Preprocess exoplanet data for machine learning\"\"\"\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()  # For all features\n",
        "        self.stellar_scaler = StandardScaler()  # For stellar features only\n",
        "        self.feature_columns = []\n",
        "        self.scaler_file = 'scaler.pkl'\n",
        "        self.stellar_scaler_file = 'stellar_scaler.pkl'\n",
        "\n",
        "    def create_labels(self, datasets):\n",
        "        \"\"\"Create binary labels for classification\"\"\"\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "        exominer_data['label'] = (exominer_data['ExoMiner Score'] > 0.7).astype(int)\n",
        "        print(\"âœ… Labels created\")\n",
        "        print(f\"ExoMiner - Planets: {exominer_data['label'].sum()}, \"\n",
        "              f\"Non-planets: {len(exominer_data) - exominer_data['label'].sum()}\")\n",
        "        return datasets\n",
        "\n",
        "    def engineer_features(self, datasets):\n",
        "        \"\"\"Create features from available data\"\"\"\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "\n",
        "        # Define feature columns\n",
        "        transit_columns = [\n",
        "            'Orbital Period [day]',\n",
        "            'Transit Duration [hour]',\n",
        "            'Transit Depth [ppm]',\n",
        "            'Planet Radius [Earth Radii]',\n",
        "            'MES',\n",
        "            'Transit Model SNR',\n",
        "            'Number of transits observed'\n",
        "        ]\n",
        "        stellar_columns = [\n",
        "            'Stellar Effective Temperature [K]',\n",
        "            'Stellar Radius [Solar Radii]',\n",
        "            'Stellar Surface Gravity [log10(cm/s*2)]'\n",
        "        ]\n",
        "        feature_columns = transit_columns + stellar_columns\n",
        "        features = exominer_data[feature_columns].copy()\n",
        "\n",
        "        # Impute missing\n",
        "        features = features.fillna(features.mean())\n",
        "\n",
        "        # Store original index\n",
        "        original_index = features.index\n",
        "\n",
        "        # Remove outliers\n",
        "        features_cleaned = self.remove_outliers_iqr(features, threshold=2.0)\n",
        "\n",
        "        # Cleaned index\n",
        "        cleaned_index = features_cleaned.index\n",
        "\n",
        "        # Derived features\n",
        "        features_cleaned['depth_duration_ratio'] = features_cleaned['Transit Depth [ppm]'] / features_cleaned['Transit Duration [hour]']\n",
        "        features_cleaned['period_snr_ratio'] = features_cleaned['Orbital Period [day]'] / features_cleaned['Transit Model SNR']\n",
        "\n",
        "        # Scale stellar features separately\n",
        "        stellar_features = features_cleaned[stellar_columns]\n",
        "        stellar_scaled = self.stellar_scaler.fit_transform(stellar_features)\n",
        "        with open(self.stellar_scaler_file, 'wb') as f:\n",
        "            pickle.dump(self.stellar_scaler, f)\n",
        "\n",
        "        # Scale all features\n",
        "        feature_names = features_cleaned.columns.tolist()\n",
        "        features_scaled = self.scaler.fit_transform(features_cleaned)\n",
        "        with open(self.scaler_file, 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "\n",
        "        features_df = pd.DataFrame(features_scaled, columns=feature_names, index=cleaned_index)\n",
        "        self.feature_columns = feature_names\n",
        "\n",
        "        datasets['ExoMiner_Features'] = features_df\n",
        "        print(\"âœ… Features engineered\")\n",
        "        print(f\"Original samples: {len(original_index)}, After cleaning: {len(cleaned_index)}\")\n",
        "        print(f\"Feature columns: {self.feature_columns}\")\n",
        "        return datasets, cleaned_index\n",
        "\n",
        "    def remove_outliers_iqr(self, df, threshold=2.0):\n",
        "        \"\"\"Remove outliers using Interquartile Range method\"\"\"\n",
        "        clean_df = df.copy()\n",
        "        for column in df.columns:\n",
        "            if df[column].dtype in ['float64', 'int64']:\n",
        "                Q1 = df[column].quantile(0.25)\n",
        "                Q3 = df[column].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - threshold * IQR\n",
        "                upper_bound = Q3 + threshold * IQR\n",
        "                mask = (clean_df[column] >= lower_bound) & (clean_df[column] <= upper_bound)\n",
        "                clean_df = clean_df[mask]\n",
        "        return clean_df\n",
        "\n",
        "## LIGHT CURVE PROCESSING\n",
        "class LightCurveProcessor:\n",
        "    \"\"\"Process light curves for AstroNet input with global/local views\"\"\"\n",
        "    def __init__(self, global_length=2001, local_length=201):\n",
        "        self.global_length = global_length\n",
        "        self.local_length = local_length\n",
        "\n",
        "    def download_single_lightcurve(self, tic_id):\n",
        "        \"\"\"Download TESS light curve for a given TIC ID (with caching)\"\"\"\n",
        "        cache_file = f\"lightcurve_{tic_id}.pkl\"\n",
        "        if os.path.exists(cache_file):\n",
        "            try:\n",
        "                return pd.read_pickle(cache_file)\n",
        "            except Exception as e:\n",
        "                print(f\"âš  Cache read error for TIC {tic_id}: {e}\")\n",
        "                os.remove(cache_file)\n",
        "\n",
        "        try:\n",
        "            search_result = lk.search_lightcurve(f\"TIC {tic_id}\", mission='TESS', author='SPOC')\n",
        "            if len(search_result) == 0:\n",
        "                print(f\"âŒ No light curves found for TIC {tic_id}\")\n",
        "                return None\n",
        "\n",
        "            lc_collection = search_result.download_all()\n",
        "            if lc_collection is None or len(lc_collection) == 0:\n",
        "                print(f\"âŒ No data downloaded for TIC {tic_id}\")\n",
        "                return None\n",
        "\n",
        "            lc = lc_collection.stitch().remove_nans().normalize()\n",
        "\n",
        "            time = np.array(lc.time.value, dtype=float)\n",
        "            flux = np.array(lc.flux.value, dtype=float)\n",
        "            flux_err = np.array(lc.flux_err.value, dtype=float) if lc.flux_err is not None else np.full_like(flux, np.nan)\n",
        "\n",
        "            lc_data = {'tic_id': tic_id, 'time': time, 'flux': flux, 'flux_err': flux_err}\n",
        "            pd.to_pickle(lc_data, cache_file)\n",
        "\n",
        "            print(f\"âœ… Downloaded TIC {tic_id} with {len(time)} points\")\n",
        "            return lc_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Download error for TIC {tic_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def preprocess_lightcurve(self, time, flux, period=None, detrend=True, normalize=True, augment=False):\n",
        "        \"\"\"Preprocess single light curve\"\"\"\n",
        "        mask = np.isfinite(time) & np.isfinite(flux)\n",
        "        time_clean, flux_clean = time[mask], flux[mask]\n",
        "        if len(flux_clean) < 100:\n",
        "            return None, None\n",
        "\n",
        "        if normalize:\n",
        "            flux_clean = flux_clean / np.median(flux_clean)\n",
        "\n",
        "        if detrend and len(flux_clean) > 101:\n",
        "            try:\n",
        "                window_length = min(101, len(flux_clean) - 1)\n",
        "                if window_length % 2 == 0:\n",
        "                    window_length -= 1\n",
        "                trend = savgol_filter(flux_clean, window_length, 2)\n",
        "                flux_clean /= trend\n",
        "            except Exception as e:\n",
        "                print(f\"âš  Detrending failed: {e}\")\n",
        "\n",
        "        if augment:\n",
        "            flux_clean += np.random.normal(0, 0.01 * np.std(flux_clean), len(flux_clean))\n",
        "\n",
        "        # Create global view (full light curve)\n",
        "        global_view = self.resample_to_fixed_length(flux_clean, self.global_length)\n",
        "\n",
        "        # Create local view (zoomed around center or mid transit)\n",
        "        mid_idx = len(flux_clean) // 2\n",
        "        half_window = self.local_length // 2\n",
        "        start = max(0, mid_idx - half_window)\n",
        "        end = min(len(flux_clean), mid_idx + half_window)\n",
        "        local_flux = flux_clean[start:end]\n",
        "        local_view = self.resample_to_fixed_length(local_flux, self.local_length)\n",
        "\n",
        "        return global_view, local_view\n",
        "\n",
        "    def resample_to_fixed_length(self, flux, length):\n",
        "        \"\"\"Resample flux to a fixed sequence length\"\"\"\n",
        "        if len(flux) == length:\n",
        "            return flux\n",
        "        x_old = np.linspace(0, 1, len(flux))\n",
        "        x_new = np.linspace(0, 1, length)\n",
        "        interp = interp1d(x_old, flux, kind='linear', fill_value='extrapolate')\n",
        "        return interp(x_new)\n",
        "def preprocess_lightcurve(self, time, flux, period=None, detrend=True, normalize=True, augment=False):\n",
        "    \"\"\"Preprocess single light curve\"\"\"\n",
        "    mask = np.isfinite(time) & np.isfinite(flux)\n",
        "    time_clean, flux_clean = time[mask], flux[mask]\n",
        "    if len(flux_clean) < 100:\n",
        "        return None, None\n",
        "\n",
        "    if normalize:\n",
        "        flux_clean = flux_clean / np.median(flux_clean)\n",
        "\n",
        "    if detrend and len(flux_clean) > 101:\n",
        "        try:\n",
        "            window_length = min(101, len(flux_clean) - 1)\n",
        "            if window_length % 2 == 0:\n",
        "                window_length -= 1\n",
        "            trend = savgol_filter(flux_clean, window_length, 2)\n",
        "            flux_clean /= trend\n",
        "        except Exception as e:\n",
        "            print(f\"âš  Detrending failed: {e}\")\n",
        "\n",
        "    if augment:\n",
        "        flux_clean += np.random.normal(0, 0.01 * np.std(flux_clean), len(flux_clean))\n",
        "\n",
        "    # Create global view (full light curve)\n",
        "    global_view = self.resample_to_fixed_length(flux_clean, self.global_length)\n",
        "\n",
        "    # Create local view (zoomed around center or mid transit)\n",
        "    mid_idx = len(flux_clean) // 2\n",
        "    half_window = self.local_length // 2\n",
        "    start = max(0, mid_idx - half_window)\n",
        "    end = min(len(flux_clean), mid_idx + half_window)\n",
        "    local_flux = flux_clean[start:end]\n",
        "    local_view = self.resample_to_fixed_length(local_flux, self.local_length)\n",
        "\n",
        "    return global_view, local_view\n",
        "    def resample_to_fixed_length(self, flux, length):\n",
        "        \"\"\"Resample flux to a fixed sequence length\"\"\"\n",
        "        if len(flux) == length:\n",
        "            return flux\n",
        "        x_old = np.linspace(0, 1, len(flux))\n",
        "        x_new = np.linspace(0, 1, length)\n",
        "        interp = interp1d(x_old, flux, kind='linear', fill_value='extrapolate')\n",
        "        return interp(x_new)\n",
        "\n",
        "#Transfer Learning\n",
        "class ExoplanetTransferLearningModel:\n",
        "    \"\"\"Improved AstroNet-like model\"\"\"\n",
        "    def __init__(self, global_shape=(2001, 1), local_shape=(201, 1), stellar_shape=(3,), num_classes=1):\n",
        "        self.global_shape = global_shape\n",
        "        self.local_shape = local_shape\n",
        "        self.stellar_shape = stellar_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = None\n",
        "\n",
        "    def create_model(self):\n",
        "        global_input = layers.Input(shape=self.global_shape, name='global_lc')\n",
        "        local_input = layers.Input(shape=self.local_shape, name='local_lc')\n",
        "        stellar_input = layers.Input(shape=self.stellar_shape, name='stellar_params')\n",
        "\n",
        "        def cnn_branch(input_layer, filters=[16, 32, 64, 128, 256]):\n",
        "            x = input_layer\n",
        "            for f in filters:\n",
        "                x = layers.Conv1D(f, 5, activation='relu', padding='same')(x)\n",
        "                x = layers.MaxPooling1D(5, strides=2, padding='same')(x)\n",
        "            x = layers.Flatten()(x)\n",
        "            return x\n",
        "\n",
        "        x_global = cnn_branch(global_input)\n",
        "        x_local = cnn_branch(local_input)\n",
        "\n",
        "        x = layers.Concatenate()([x_global, x_local])\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        residual = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Add()([x, residual])\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "\n",
        "        s = layers.Dense(32, activation='relu')(stellar_input)\n",
        "        x = layers.Concatenate()([x, s])\n",
        "        x = layers.Dense(128, activation='relu')(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "\n",
        "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "        self.model = keras.Model(inputs=[global_input, local_input, stellar_input], outputs=outputs)\n",
        "        return self.model\n",
        "\n",
        "    def compile_model(self, learning_rate=0.001):\n",
        "        self.model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate),\n",
        "            loss=BinaryFocalCrossentropy(alpha=0.25, gamma=2.0),\n",
        "            metrics=['accuracy', keras.metrics.Precision(name='precision'),\n",
        "                     keras.metrics.Recall(name='recall'), keras.metrics.AUC(name='auc')]\n",
        "        )\n",
        "        print(\"âœ… Model compiled with focal loss\")\n",
        "\n",
        "    def setup_callbacks(self):\n",
        "        return [\n",
        "            callbacks.EarlyStopping(monitor='val_auc', patience=20, mode='max', restore_best_weights=True),\n",
        "            callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=10, min_lr=1e-6, mode='max'),\n",
        "            callbacks.ModelCheckpoint('best_model.h5', monitor='val_auc', save_best_only=True, mode='max')\n",
        "        ]\n",
        "\n",
        "#Model Training and Evaluation\n",
        "class ModelTrainer:\n",
        "    \"\"\"Train and evaluate the exoplanet detection model\"\"\"\n",
        "    def __init__(self):\n",
        "        self.history = None\n",
        "\n",
        "    def prepare_lc_datasets(self, datasets, cleaned_index, processor, preprocessor):\n",
        "        global_lc = []\n",
        "        local_lc = []\n",
        "        stellar = []\n",
        "        labels = []\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "\n",
        "        for idx in cleaned_index:\n",
        "            tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "            period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "            label = exominer_data.loc[idx, 'label']\n",
        "            lc_data = processor.download_single_lightcurve(tic_id)\n",
        "            if lc_data is not None:\n",
        "                g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "                if g_flux is not None and l_flux is not None:\n",
        "                    global_lc.append(g_flux)\n",
        "                    local_lc.append(l_flux)\n",
        "                    stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]',\n",
        "                                                         'Stellar Radius [Solar Radii]',\n",
        "                                                         'Stellar Surface Gravity [log10(cm/s*2)]']].values\n",
        "                    stellar_row = np.nan_to_num(stellar_row)\n",
        "\n",
        "                    # Scale stellar features using the stellar scaler\n",
        "                    try:\n",
        "                        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "                            scaler = pickle.load(f)\n",
        "                        stellar_row = scaler.transform(stellar_row.reshape(1, -1)).flatten()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading stellar scaler for TIC {tic_id}: {e}. Using raw stellar features.\")\n",
        "                        stellar_row = stellar_row  # Fallback to raw features\n",
        "                    stellar.append(stellar_row)\n",
        "                    labels.append(label)\n",
        "\n",
        "        if not global_lc:\n",
        "            raise ValueError(\"No valid light curves processed.\")\n",
        "\n",
        "        global_lc = np.array(global_lc).reshape(-1, processor.global_length, 1)\n",
        "        local_lc = np.array(local_lc).reshape(-1, processor.local_length, 1)\n",
        "        stellar = np.array(stellar)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        X_train_g, X_test_g, X_train_l, X_test_l, X_train_s, X_test_s, y_train, y_test = train_test_split(\n",
        "            global_lc, local_lc, stellar, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "        )\n",
        "        X_train_g, X_val_g, X_train_l, X_val_l, X_train_s, X_val_s, y_train, y_val = train_test_split(\n",
        "            X_train_g, X_train_l, X_train_s, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        "        )\n",
        "\n",
        "        return X_train_g, X_val_g, X_test_g, X_train_l, X_val_l, X_test_l, X_train_s, X_val_s, X_test_s, y_train, y_val, y_test\n",
        "\n",
        "    def compute_class_weights(self, y_train):\n",
        "        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "        weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "        print(f\"ðŸ“Š Class weights: {weight_dict}\")\n",
        "        return weight_dict\n",
        "\n",
        "    def train_model(self, model, X_train_g, X_train_l, X_train_s, y_train, X_val_g, X_val_l, X_val_s, y_val,\n",
        "                    epochs=200, batch_size=64, class_weight=None):\n",
        "        callbacks = model.setup_callbacks()\n",
        "        self.history = model.model.fit(\n",
        "            [X_train_g, X_train_l, X_train_s], y_train,\n",
        "            validation_data=([X_val_g, X_val_l, X_val_s], y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            class_weight=class_weight,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "        return self.history\n",
        "\n",
        "    def evaluate_model(self, model, X_test_g, X_test_l, X_test_s, y_test):\n",
        "        model.model = keras.models.load_model('best_model.h5')\n",
        "        y_pred_proba = model.model.predict([X_test_g, X_test_l, X_test_s], verbose=0)\n",
        "        y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "        test_accuracy = accuracy_score(y_test, y_pred)\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        print(f\"\\nðŸ“ˆ TEST RESULTS:\")\n",
        "        print(f\" Accuracy: {test_accuracy:.4f}\")\n",
        "        print(f\" ROC AUC: {roc_auc:.4f}\")\n",
        "        print(f\" Precision: {report['1']['precision']:.4f}\")\n",
        "        print(f\" Recall: {report['1']['recall']:.4f}\")\n",
        "        return y_pred, y_pred_proba\n",
        "\n",
        "    def cross_validate(self, model_class, datasets, cleaned_index, processor, preprocessor, n_splits=5):\n",
        "        auc_scores = []\n",
        "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "        labels = datasets['ExoMiner']['label'].loc[cleaned_index]\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
        "            print(f\"Cross-validation Fold {fold+1}/{n_splits}\")\n",
        "            train_cleaned = cleaned_index[train_idx]\n",
        "            val_cleaned = cleaned_index[val_idx]\n",
        "            X_train_g, X_val_g, X_train_l, X_val_l, X_train_s, X_val_s, y_train, y_val = \\\n",
        "                self.prepare_lc_datasets_for_cv(datasets, train_cleaned, val_cleaned, processor, preprocessor)\n",
        "\n",
        "            model = model_class()\n",
        "            model.create_model()\n",
        "            model.compile_model()\n",
        "            self.train_model(model, X_train_g, X_train_l, X_train_s, y_train,\n",
        "                           X_val_g, X_val_l, X_val_s, y_val, epochs=50)\n",
        "            y_pred_proba = model.model.predict([X_val_g, X_val_l, X_val_s], verbose=0)\n",
        "            fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
        "            auc_scores.append(auc(fpr, tpr))\n",
        "\n",
        "        print(f\"\\nCross-validation AUC: {np.mean(auc_scores):.3f} Â± {np.std(auc_scores):.3f}\")\n",
        "        return auc_scores\n",
        "\n",
        "    def prepare_lc_datasets_for_cv(self, datasets, train_index, val_index, processor, preprocessor):\n",
        "        global_lc_train = []\n",
        "        local_lc_train = []\n",
        "        stellar_train = []\n",
        "        labels_train = []\n",
        "        global_lc_val = []\n",
        "        local_lc_val = []\n",
        "        stellar_val = []\n",
        "        labels_val = []\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "\n",
        "        # Process training data\n",
        "        for idx in train_index:\n",
        "            tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "            period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "            label = exominer_data.loc[idx, 'label']\n",
        "            lc_data = processor.download_single_lightcurve(tic_id)\n",
        "            if lc_data is not None:\n",
        "                g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "                if g_flux is not None and l_flux is not None:\n",
        "                    global_lc_train.append(g_flux)\n",
        "                    local_lc_train.append(l_flux)\n",
        "                    stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]',\n",
        "                                                         'Stellar Radius [Solar Radii]',\n",
        "                                                         'Stellar Surface Gravity [log10(cm/s*2)]']].values\n",
        "                    stellar_row = np.nan_to_num(stellar_row)\n",
        "                    try:\n",
        "                        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "                            scaler = pickle.load(f)\n",
        "                        stellar_row = scaler.transform(stellar_row.reshape(1, -1)).flatten()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading stellar scaler for TIC {tic_id}: {e}. Using raw stellar features.\")\n",
        "                        stellar_row = stellar_row\n",
        "                    stellar_train.append(stellar_row)\n",
        "                    labels_train.append(label)\n",
        "\n",
        "        # Process validation data\n",
        "        for idx in val_index:\n",
        "            tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "            period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "            label = exominer_data.loc[idx, 'label']\n",
        "            lc_data = processor.download_single_lightcurve(tic_id)\n",
        "            if lc_data is not None:\n",
        "                g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "                if g_flux is not None and l_flux is not None:\n",
        "                    global_lc_val.append(g_flux)\n",
        "                    local_lc_val.append(l_flux)\n",
        "                    stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]',\n",
        "                                                         'Stellar Radius [Solar Radii]',\n",
        "                                                         'Stellar Surface Gravity [log10(cm/s*2)]']].values\n",
        "                    stellar_row = np.nan_to_num(stellar_row)\n",
        "                    try:\n",
        "                        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "                            scaler = pickle.load(f)\n",
        "                        stellar_row = scaler.transform(stellar_row.reshape(1, -1)).flatten()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading stellar scaler for TIC {tic_id}: {e}. Using raw stellar features.\")\n",
        "                        stellar_row = stellar_row\n",
        "                    stellar_val.append(stellar_row)\n",
        "                    labels_val.append(label)\n",
        "\n",
        "        if not global_lc_train or not global_lc_val:\n",
        "            raise ValueError(\"No valid light curves processed for cross-validation.\")\n",
        "\n",
        "        X_train_g = np.array(global_lc_train).reshape(-1, processor.global_length, 1)\n",
        "        X_train_l = np.array(local_lc_train).reshape(-1, processor.local_length, 1)\n",
        "        X_train_s = np.array(stellar_train)\n",
        "        y_train = np.array(labels_train)\n",
        "        X_val_g = np.array(global_lc_val).reshape(-1, processor.global_length, 1)\n",
        "        X_val_l = np.array(local_lc_val).reshape(-1, processor.local_length, 1)\n",
        "        X_val_s = np.array(stellar_val)\n",
        "        y_val = np.array(labels_val)\n",
        "\n",
        "        return X_train_g, X_val_g, X_train_l, X_val_l, X_train_s, X_val_s, y_train, y_val\n",
        "\n",
        "#Hyper Parameter Tuning\n",
        "def tune_hyperparameters(X_train_g, X_train_l, X_train_s, y_train, X_val_g, X_val_l, X_val_s, y_val):\n",
        "    \"\"\"Tune hyperparameters using Keras Tuner\"\"\"\n",
        "    def build_model(hp):\n",
        "        model = ExoplanetTransferLearningModel()\n",
        "        model.create_model()\n",
        "        model.compile_model(learning_rate=hp.Float('lr', 1e-5, 1e-3, sampling='log'))\n",
        "        return model.model\n",
        "\n",
        "    tuner = kt.Hyperband(\n",
        "        build_model,\n",
        "        objective='val_auc',\n",
        "        max_epochs=200,\n",
        "        factor=3,\n",
        "        directory='tuner_dir',\n",
        "        project_name='exoplanet'\n",
        "    )\n",
        "\n",
        "    tuner.search([X_train_g, X_train_l, X_train_s], y_train,\n",
        "                 validation_data=([X_val_g, X_val_l, X_val_s], y_val),\n",
        "                 epochs=50)\n",
        "\n",
        "    best_hps = tuner.get_best_hyperparameters()[0]\n",
        "    print(f\"Best learning rate: {best_hps.get('lr')}\")\n",
        "    return tuner.get_best_models(1)[0], best_hps\n",
        "\n",
        "# ============================================\n",
        "# 7. VISUALIZATION AND ANALYSIS\n",
        "# ============================================\n",
        "def visualize_results(history, y_test, y_pred, y_pred_proba):\n",
        "    \"\"\"Visualize training history and evaluation metrics\"\"\"\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history.history['accuracy'], label='accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "    plt.plot(history.history['auc'], label='auc')\n",
        "    plt.plot(history.history['val_auc'], label='val_auc')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Metric Value')\n",
        "    plt.title('Training History')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot ROC curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=['Not Exoplanet', 'Exoplanet'],\n",
        "                yticklabels=['Not Exoplanet', 'Exoplanet'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "def test_with_real_lightcurves(model, datasets, preprocessor, num_samples=10):\n",
        "    \"\"\"Test the model with real light curves not used in training\"\"\"\n",
        "    print(f\"\\nðŸ”¬ Testing with {num_samples} real light curves:\")\n",
        "    exominer_data = datasets['ExoMiner']\n",
        "    all_indices = exominer_data.index\n",
        "    cleaned_index = datasets['ExoMiner_Features'].index\n",
        "    unseen_indices = all_indices.difference(cleaned_index)\n",
        "\n",
        "    if len(unseen_indices) < num_samples:\n",
        "        print(\"Not enough unseen samples to test.\")\n",
        "        return\n",
        "\n",
        "    test_indices = np.random.choice(unseen_indices, num_samples, replace=False)\n",
        "    processor = LightCurveProcessor()\n",
        "\n",
        "    try:\n",
        "        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "            scaler = pickle.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading stellar scaler: {e}. Using raw stellar features.\")\n",
        "        scaler = None\n",
        "\n",
        "    for idx in test_indices:\n",
        "        tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "        period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "        true_label = exominer_data.loc[idx, 'label']\n",
        "        lc_data = processor.download_single_lightcurve(tic_id)\n",
        "        if lc_data is not None:\n",
        "            g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "            if g_flux is not None and l_flux is not None:\n",
        "                stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]',\n",
        "                                                    'Stellar Radius [Solar Radii]',\n",
        "                                                    'Stellar Surface Gravity [log10(cm/s*2)]']].values\n",
        "                stellar_row = np.nan_to_num(stellar_row).reshape(1, -1)\n",
        "                if scaler is not None:\n",
        "                    stellar_row = scaler.transform(stellar_row).flatten()\n",
        "                g_flux = np.array(g_flux).reshape(1, processor.global_length, 1)\n",
        "                l_flux = np.array(l_flux).reshape(1, processor.local_length, 1)\n",
        "                y_pred_proba = model.model.predict([g_flux, l_flux, stellar_row], verbose=0)\n",
        "                predicted_label = (y_pred_proba > 0.5).astype(int).flatten()[0]\n",
        "                print(f\" TIC ID: {tic_id}, True Label: {true_label}, \"\n",
        "                      f\"Predicted Probability: {y_pred_proba[0][0]:.4f}, Predicted Label: {predicted_label}\")\n",
        "            else:\n",
        "                print(f\" TIC ID: {tic_id} - Preprocessing failed.\")\n",
        "        else:\n",
        "            print(f\" TIC ID: {tic_id} - Light curve download failed.\")\n",
        "\n",
        "def analyze_feature_importance(datasets):\n",
        "    \"\"\"Analyze feature importance using RandomForest\"\"\"\n",
        "    print(\"\\nðŸ“Š Analyzing Feature Importance:\")\n",
        "    exominer_features = datasets['ExoMiner_Features']\n",
        "    labels = datasets['ExoMiner'].loc[exominer_features.index, 'label']\n",
        "\n",
        "    if len(exominer_features) == 0 or len(labels) == 0:\n",
        "        print(\"Not enough data to analyze feature importance.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            exominer_features, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "        )\n",
        "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        rf_model.fit(X_train, y_train)\n",
        "        importances = rf_model.feature_importances_\n",
        "        feature_names = exominer_features.columns\n",
        "        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "        feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
        "        print(\"Top 10 Feature Importance:\")\n",
        "        print(feature_importance_df.head(10))\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x='importance', y='feature', data=feature_importance_df.head(10), palette='viridis')\n",
        "        plt.title('Top 10 Feature Importance from RandomForest')\n",
        "        plt.xlabel('Importance')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during feature importance analysis: {e}\")\n",
        "\n",
        "#Execution of pipeline\n",
        "def main():\n",
        "    \"\"\"Execute the exoplanet detection pipeline\"\"\"\n",
        "    datasets = load_and_parse_datasets()\n",
        "    preprocessor = ExoplanetDataPreprocessor()\n",
        "    datasets = preprocessor.create_labels(datasets)\n",
        "    datasets, cleaned_index = preprocessor.engineer_features(datasets)\n",
        "    processor = LightCurveProcessor()\n",
        "    trainer = ModelTrainer()\n",
        "\n",
        "    X_train_g, X_val_g, X_test_g, X_train_l, X_val_l, X_test_l, X_train_s, X_val_s, X_test_s, y_train, y_val, y_test = \\\n",
        "        trainer.prepare_lc_datasets(datasets, cleaned_index, processor, preprocessor)\n",
        "\n",
        "    best_model_hp, best_hps = tune_hyperparameters(\n",
        "        X_train_g, X_train_l, X_train_s, y_train, X_val_g, X_val_l, X_val_s, y_val\n",
        "    )\n",
        "\n",
        "    # Re-create and train the final model\n",
        "    model = ExoplanetTransferLearningModel()\n",
        "    model.create_model()\n",
        "    model.compile_model(learning_rate=best_hps.get('lr'))\n",
        "\n",
        "    X_train_g_full = np.concatenate((X_train_g, X_val_g))\n",
        "    X_train_l_full = np.concatenate((X_train_l, X_val_l))\n",
        "    X_train_s_full = np.concatenate((X_train_s, X_val_s))\n",
        "    y_train_full = np.concatenate((y_train, y_val))\n",
        "\n",
        "    class_weights = trainer.compute_class_weights(y_train_full)\n",
        "    history = trainer.train_model(\n",
        "        model, X_train_g_full, X_train_l_full, X_train_s_full, y_train_full,\n",
        "        X_test_g, X_test_l, X_test_s, y_test, epochs=200, batch_size=64, class_weight=class_weights\n",
        "    )\n",
        "\n",
        "    y_pred, y_pred_proba = trainer.evaluate_model(model, X_test_g, X_test_l, X_test_s, y_test)\n",
        "\n",
        "    visualize_results(history, y_test, y_pred, y_pred_proba)\n",
        "    test_with_real_lightcurves(model, datasets, preprocessor, num_samples=10)\n",
        "    analyze_feature_importance(datasets)\n",
        "    auc_scores = trainer.cross_validate(ExoplanetTransferLearningModel, datasets, cleaned_index, processor, preprocessor)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        model, history = main()\n",
        "        print(\"\\nðŸŽ‰ PIPELINE COMPLETED!\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfVTOzwLXWNE",
        "outputId": "71851f85-90d2-4ca0-a9d9-2dca67e3f5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lightkurve\n",
            "  Downloading lightkurve-2.5.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: astropy>=5.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (7.1.0)\n",
            "Collecting astroquery>=0.3.10 (from lightkurve)\n",
            "  Downloading astroquery-0.4.11-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (4.13.5)\n",
            "Requirement already satisfied: bokeh>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (3.7.3)\n",
            "Collecting fbpca>=1.0 (from lightkurve)\n",
            "  Downloading fbpca-1.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.1 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (3.10.0)\n",
            "Collecting memoization>=0.3.1 (from lightkurve)\n",
            "  Downloading memoization-0.4.0.tar.gz (41 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.3.6 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (1.0.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (2.32.4)\n",
            "Collecting s3fs>=2024.6.1 (from lightkurve)\n",
            "  Downloading s3fs-2025.9.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (1.16.2)\n",
            "Requirement already satisfied: tqdm>=4.25.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (4.67.1)\n",
            "Collecting uncertainties>=3.1.4 (from lightkurve)\n",
            "  Downloading uncertainties-3.2.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: urllib3>=1.23 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (2.5.0)\n",
            "Requirement already satisfied: pyerfa>=2.0.1.1 in /usr/local/lib/python3.12/dist-packages (from astropy>=5.0->lightkurve) (2.0.1.5)\n",
            "Requirement already satisfied: astropy-iers-data>=0.2025.4.28.0.37.27 in /usr/local/lib/python3.12/dist-packages (from astropy>=5.0->lightkurve) (0.2025.9.29.0.35.48)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from astropy>=5.0->lightkurve) (6.0.3)\n",
            "Requirement already satisfied: packaging>=22.0.0 in /usr/local/lib/python3.12/dist-packages (from astropy>=5.0->lightkurve) (25.0)\n",
            "Requirement already satisfied: html5lib>=0.999 in /usr/local/lib/python3.12/dist-packages (from astroquery>=0.3.10->lightkurve) (1.1)\n",
            "Requirement already satisfied: keyring>=15.0 in /usr/local/lib/python3.12/dist-packages (from astroquery>=0.3.10->lightkurve) (25.6.0)\n",
            "Collecting pyvo>=1.5 (from astroquery>=0.3.10->lightkurve)\n",
            "  Downloading pyvo-1.7-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.6.0->lightkurve) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.6.0->lightkurve) (4.15.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (3.1.6)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (1.3.3)\n",
            "Requirement already satisfied: narwhals>=1.13 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (2.6.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (11.3.0)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (2025.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.6->lightkurve) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.6->lightkurve) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->lightkurve) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->lightkurve) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->lightkurve) (2025.8.3)\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading aiobotocore-2.24.2-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting fsspec==2025.9.0 (from s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from s3fs>=2024.6.1->lightkurve) (3.12.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->lightkurve) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->lightkurve) (3.6.0)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting botocore<1.40.19,>=1.40.15 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading botocore-1.40.18-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve) (6.6.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (1.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (1.20.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.12/dist-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=2.9->bokeh>=2.3.2->lightkurve) (3.0.3)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (3.4.0)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (0.9.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (3.4.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (4.3.0)\n",
            "Requirement already satisfied: jaraco.context in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (6.0.1)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.12/dist-packages (from SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (43.0.3)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from jaraco.classes->keyring>=15.0->astroquery>=0.3.10->lightkurve) (10.8.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (2.23)\n",
            "Downloading lightkurve-2.5.1-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astroquery-0.4.11-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m130.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3fs-2025.9.0-py3-none-any.whl (30 kB)\n",
            "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uncertainties-3.2.3-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiobotocore-2.24.2-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyvo-1.7-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading botocore-1.40.18-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: fbpca, memoization\n",
            "  Building wheel for fbpca (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fbpca: filename=fbpca-1.0-py3-none-any.whl size=11373 sha256=d03888548da6ad2f7e101d2890564500a96c093d9ad2cfd5f9c1cfdf0ccfa36e\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/15/cd/2f622795b09e83471a3be5d2581cd9cf96a6ec7aa78e8deffe\n",
            "  Building wheel for memoization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memoization: filename=memoization-0.4.0-py3-none-any.whl size=50452 sha256=611a8e0286cdaadff681be1e8ecd0f6d0e2f016ee3f0bed424feb165990d36b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/35/02/90618fc7cbf03a335f3cacd59d32b35930bf5a57f3c0d0814c\n",
            "Successfully built fbpca memoization\n",
            "Installing collected packages: fbpca, uncertainties, memoization, jmespath, fsspec, aioitertools, botocore, pyvo, aiobotocore, s3fs, astroquery, lightkurve\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiobotocore-2.24.2 aioitertools-0.12.0 astroquery-0.4.11 botocore-1.40.18 fbpca-1.0 fsspec-2025.9.0 jmespath-1.0.1 lightkurve-2.5.1 memoization-0.4.0 pyvo-1.7 s3fs-2025.9.0 uncertainties-3.2.3\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Collecting keras_tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (from keras_tuner) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras_tuner) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from keras_tuner) (2.32.4)\n",
            "Collecting kt-legacy (from keras_tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (0.5.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras->keras_tuner) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras_tuner) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras_tuner) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras_tuner\n",
            "Successfully installed keras_tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightkurve/prf/__init__.py:7: UserWarning: Warning: the tpfmodel submodule is not available without oktopus installed, which requires a current version of autograd. See #1452 for details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Libraries imported successfully\n",
            "TensorFlow version: 2.19.0\n",
            "Lightkurve version: 2.5.1\n",
            "Loading K2 dataset...\n",
            "Loading ExoMiner dataset...\n",
            "Loading TOI dataset...\n",
            "Merging ExoMiner with TOI...\n",
            "Loading Cumulative dataset...\n",
            "âœ… Labels created\n",
            "ExoMiner - Planets: 7126, Non-planets: 5641\n",
            "âœ… Features engineered\n",
            "Original samples: 12767, After cleaning: 8226\n",
            "Feature columns: ['Orbital Period [day]', 'Transit Duration [hour]', 'Transit Depth [ppm]', 'Planet Radius [Earth Radii]', 'MES', 'Transit Model SNR', 'Number of transits observed', 'Stellar Effective Temperature [K]', 'Stellar Radius [Solar Radii]', 'Stellar Surface Gravity [log10(cm/s*2)]', 'depth_duration_ratio', 'period_snr_ratio']\n",
            "âœ… Downloaded TIC 239332587 with 690692 points\n",
            "âœ… Downloaded TIC 24695044 with 376102 points\n",
            "âœ… Downloaded TIC 416195870 with 1464070 points\n",
            "âœ… Downloaded TIC 366115856 with 84588 points\n",
            "âœ… Downloaded TIC 232612416 with 918935 points\n",
            "âœ… Downloaded TIC 406941612 with 105284 points\n",
            "âœ… Downloaded TIC 300604770 with 267353 points\n",
            "âœ… Downloaded TIC 153078576 with 33495 points\n",
            "âœ… Downloaded TIC 153951307 with 307043 points\n",
            "âœ… Downloaded TIC 158002130 with 245917 points\n",
            "âœ… Downloaded TIC 220475245 with 470426 points\n",
            "âœ… Downloaded TIC 359271092 with 645666 points\n",
            "âœ… Downloaded TIC 229605891 with 413015 points\n",
            "âœ… Downloaded TIC 441763252 with 384656 points\n",
            "âœ… Downloaded TIC 199444169 with 152581 points\n",
            "âœ… Downloaded TIC 233823679 with 129127 points\n",
            "âœ… Downloaded TIC 233087860 with 1824953 points\n",
            "âœ… Downloaded TIC 160390955 with 66914 points\n",
            "âœ… Downloaded TIC 101696403 with 12827 points\n",
            "âœ… Downloaded TIC 219857012 with 3170779 points\n",
            "âœ… Downloaded TIC 260304800 with 635018 points\n",
            "âœ… Downloaded TIC 123357034 with 130218 points\n",
            "âœ… Downloaded TIC 230017324 with 1159114 points\n",
            "âœ… Downloaded TIC 232540264 with 1374061 points\n",
            "âœ… Downloaded TIC 260417932 with 422356 points\n",
            "âœ… Downloaded TIC 309791156 with 744444 points\n",
            "âœ… Downloaded TIC 307956397 with 487824 points\n",
            "âœ… Downloaded TIC 165723070 with 62387 points\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n",
            "WARNING:lightkurve.utils:Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Downloaded TIC 382626661 with 2016809 points\n",
            "âœ… Downloaded TIC 54962195 with 62294 points\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: 30% (5881/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n",
            "WARNING:lightkurve.utils:Warning: 30% (5881/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Downloaded TIC 167415965 with 3250768 points\n",
            "âœ… Downloaded TIC 186470968 with 51913 points\n",
            "âœ… Downloaded TIC 58542531 with 259271 points\n",
            "âœ… Downloaded TIC 130924120 with 47388 points\n",
            "âœ… Downloaded TIC 272672133 with 517178 points\n",
            "âœ… Downloaded TIC 284441182 with 160904 points\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: 30% (5874/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n",
            "WARNING:lightkurve.utils:Warning: 30% (5874/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Downloaded TIC 278683844 with 3577891 points\n",
            "âœ… Downloaded TIC 415969908 with 272632 points\n",
            "âœ… Downloaded TIC 71431780 with 220172 points\n",
            "âœ… Downloaded TIC 367858035 with 175219 points\n",
            "âœ… Downloaded TIC 219508169 with 258533 points\n",
            "âœ… Downloaded TIC 237086564 with 197458 points\n",
            "âœ… Downloaded TIC 156007004 with 45486 points\n",
            "âœ… Downloaded TIC 327369524 with 90758 points\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: 24% (4655/19050) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n",
            "WARNING:lightkurve.utils:Warning: 24% (4655/19050) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Downloaded TIC 141205978 with 48123 points\n",
            "âœ… Downloaded TIC 160045097 with 597066 points\n",
            "âœ… Downloaded TIC 302381397 with 93788 points\n",
            "âœ… Downloaded TIC 198390247 with 1758881 points\n",
            "âœ… Downloaded TIC 75878355 with 456094 points\n",
            "âœ… Downloaded TIC 462615350 with 474966 points\n",
            "âœ… Downloaded TIC 142387023 with 304761 points\n",
            "âœ… Downloaded TIC 184733148 with 30356 points\n",
            "âœ… Downloaded TIC 406672232 with 298095 points\n",
            "âœ… Downloaded TIC 237099296 with 611240 points\n",
            "âœ… Downloaded TIC 161003569 with 50486 points\n",
            "âœ… Downloaded TIC 270342589 with 121358 points\n",
            "âœ… Downloaded TIC 280803917 with 131494 points\n",
            "âœ… Downloaded TIC 141768070 with 722280 points\n",
            "âœ… Downloaded TIC 77253676 with 66619 points\n",
            "âœ… Downloaded TIC 147890655 with 41343 points\n",
            "âœ… Downloaded TIC 257241363 with 772755 points\n",
            "âœ… Downloaded TIC 239134248 with 60319 points\n",
            "âœ… Downloaded TIC 272758199 with 353700 points\n",
            "âœ… Downloaded TIC 376981340 with 41595 points\n",
            "âœ… Downloaded TIC 272785423 with 355977 points\n",
            "âœ… Downloaded TIC 233188747 with 481944 points\n",
            "âœ… Downloaded TIC 467971286 with 134134 points\n",
            "âœ… Downloaded TIC 470987100 with 46943 points\n",
            "âœ… Downloaded TIC 268334473 with 645065 points\n",
            "âœ… Downloaded TIC 318022259 with 145343 points\n",
            "âœ… Downloaded TIC 452964680 with 63664 points\n",
            "âœ… Downloaded TIC 137420801 with 86942 points\n",
            "âœ… Downloaded TIC 417129824 with 49675 points\n",
            "âœ… Downloaded TIC 287591638 with 53887 points\n",
            "âœ… Downloaded TIC 317597583 with 727773 points\n",
            "âœ… Downloaded TIC 365938305 with 411045 points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eNH-3K_FXWv0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}