{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khaled-Wael/NASA_Project/blob/main/Nasa_Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#` EXOPLANET DETECTION PIPELINE`"
      ],
      "metadata": {
        "id": "-NOFweT0RhNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightkurve\n",
        "!pip install tensorflow\n",
        "!pip install keras_tuner"
      ],
      "metadata": {
        "id": "hFD-EhBjRLzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Libraries Upload"
      ],
      "metadata": {
        "id": "91gJHSQ2QeSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy import stats\n",
        "import lightkurve as lk\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, auc\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
        "import keras_tuner as kt\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"âœ… Libraries imported successfully\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Lightkurve version: {lk.__version__}\")\n"
      ],
      "metadata": {
        "id": "cquFipisQOO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "UQXafPnoQq2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. DATA LOADING AND PARSING\n",
        "# ============================================\n",
        "\n",
        "def load_and_parse_datasets():\n",
        "    \"\"\"Load and merge datasets for stellar params\"\"\"\n",
        "    datasets = {}\n",
        "    # Load K2\n",
        "    print(\"Loading K2 dataset...\")\n",
        "    with open('/content/sample_data/k2pandc_2025.09.25_19.49.42.csv', 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    header_line = None\n",
        "    for i, line in enumerate(lines):\n",
        "        if 'pl_name' in line and 'hostname' in line:\n",
        "            header_line = i\n",
        "            break\n",
        "    if header_line:\n",
        "        k2_data = pd.read_csv('/content/sample_data/k2pandc_2025.09.25_19.49.42.csv',\n",
        "                              skiprows=header_line, sep=',', engine='python')\n",
        "    else:\n",
        "        k2_data = pd.read_csv('/content/sample_data/k2pandc_2025.09.25_19.49.42.csv',\n",
        "                              skiprows=97, header=None, names=['raw_data'])\n",
        "    datasets['K2'] = k2_data\n",
        "\n",
        "    # Load ExoMiner\n",
        "    print(\"Loading ExoMiner dataset...\")\n",
        "    exominer_data = pd.read_csv('/content/sample_data/exominer_vetting_tess-spoc-2-min-s1s67_dashtable_dvm-url_scoregt0.1 (1).csv')\n",
        "    datasets['ExoMiner'] = exominer_data\n",
        "\n",
        "    # Load TOI\n",
        "    print(\"Loading TOI dataset...\")\n",
        "    try:\n",
        "        toi_data = pd.read_csv('/content/sample_data/TOI_2025.09.25_19.49.24.csv', comment='#', engine='python')\n",
        "        datasets['TOI'] = toi_data\n",
        "    except:\n",
        "        datasets['TOI'] = pd.DataFrame()\n",
        "\n",
        "    # Merge ExoMiner with TOI\n",
        "    if not datasets['TOI'].empty:\n",
        "        print(\"Merging ExoMiner with TOI...\")\n",
        "        exominer_data = exominer_data.merge(toi_data, left_on='TIC ID', right_on='tid', how='left')\n",
        "        exominer_data = exominer_data.rename(columns={\n",
        "            'st_teff': 'Stellar Effective Temperature [K]',\n",
        "            'st_rad': 'Stellar Radius [Solar Radii]',\n",
        "            'st_logg': 'Stellar Surface Gravity [log10(cm/s**2)]'\n",
        "        })\n",
        "        datasets['ExoMiner'] = exominer_data\n",
        "\n",
        "    # Load Cumulative\n",
        "    print(\"Loading Cumulative dataset...\")\n",
        "    try:\n",
        "        cumulative_data = pd.read_csv('/content/sample_data/cumulative_2025.09.25_19.49.05.csv', comment='#', engine='python')\n",
        "        datasets['Cumulative'] = cumulative_data\n",
        "    except:\n",
        "        datasets['Cumulative'] = pd.DataFrame()\n",
        "\n",
        "    return datasets"
      ],
      "metadata": {
        "id": "_N21I3ZUQxb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing and Feature Engineering"
      ],
      "metadata": {
        "id": "1tyN2UfzQz_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExoplanetDataPreprocessor:\n",
        "    \"\"\"Preprocess exoplanet data for machine learning\"\"\"\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()  # For all features\n",
        "        self.stellar_scaler = StandardScaler()  # For stellar features only\n",
        "        self.feature_columns = []\n",
        "        self.scaler_file = 'scaler.pkl'\n",
        "        self.stellar_scaler_file = 'stellar_scaler.pkl'\n",
        "\n",
        "    def create_labels(self, datasets):\n",
        "        \"\"\"Create binary labels for classification\"\"\"\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "        exominer_data['label'] = (exominer_data['ExoMiner Score'] > 0.7).astype(int)\n",
        "        print(\"âœ… Labels created\")\n",
        "        print(f\"ExoMiner - Planets: {exominer_data['label'].sum()}, \"\n",
        "              f\"Non-planets: {len(exominer_data) - exominer_data['label'].sum()}\")\n",
        "        return datasets\n",
        "\n",
        "    def engineer_features(self, datasets):\n",
        "        \"\"\"Create features from available data\"\"\"\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "        # Define feature columns\n",
        "        transit_columns = [\n",
        "            'Orbital Period [day]', 'Transit Duration [hour]',\n",
        "            'Transit Depth [ppm]', 'Planet Radius [Earth Radii]',\n",
        "            'MES', 'Transit Model SNR', 'Number of transits observed'\n",
        "        ]\n",
        "        stellar_columns = [\n",
        "            'Stellar Effective Temperature [K]', 'Stellar Radius [Solar Radii]',\n",
        "            'Stellar Surface Gravity [log10(cm/s**2)]'\n",
        "        ]\n",
        "        feature_columns = transit_columns + stellar_columns\n",
        "        features = exominer_data[feature_columns].copy()\n",
        "\n",
        "        # Impute missing\n",
        "        features = features.fillna(features.mean())\n",
        "\n",
        "        # Store original index\n",
        "        original_index = features.index\n",
        "\n",
        "        # Remove outliers\n",
        "        features_cleaned = self.remove_outliers_iqr(features, threshold=2.0)\n",
        "\n",
        "        # Cleaned index\n",
        "        cleaned_index = features_cleaned.index\n",
        "\n",
        "        # Derived features\n",
        "        features_cleaned['depth_duration_ratio'] = features_cleaned['Transit Depth [ppm]'] / features_cleaned['Transit Duration [hour]']\n",
        "        features_cleaned['period_snr_ratio'] = features_cleaned['Orbital Period [day]'] / features_cleaned['Transit Model SNR']\n",
        "\n",
        "        # Scale stellar features separately\n",
        "        stellar_features = features_cleaned[stellar_columns]\n",
        "        stellar_scaled = self.stellar_scaler.fit_transform(stellar_features)\n",
        "        with open(self.stellar_scaler_file, 'wb') as f:\n",
        "            pickle.dump(self.stellar_scaler, f)\n",
        "\n",
        "        # Scale all features\n",
        "        feature_names = features_cleaned.columns.tolist()\n",
        "        features_scaled = self.scaler.fit_transform(features_cleaned)\n",
        "        with open(self.scaler_file, 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "        features_df = pd.DataFrame(features_scaled, columns=feature_names, index=cleaned_index)\n",
        "\n",
        "        self.feature_columns = feature_names\n",
        "        datasets['ExoMiner_Features'] = features_df\n",
        "\n",
        "        print(\"âœ… Features engineered\")\n",
        "        print(f\"Original samples: {len(original_index)}, After cleaning: {len(cleaned_index)}\")\n",
        "        print(f\"Feature columns: {self.feature_columns}\")\n",
        "\n",
        "        return datasets, cleaned_index\n",
        "\n",
        "    def remove_outliers_iqr(self, df, threshold=2.0):\n",
        "        \"\"\"Remove outliers using Interquartile Range method\"\"\"\n",
        "        clean_df = df.copy()\n",
        "        for column in df.columns:\n",
        "            if df[column].dtype in ['float64', 'int64']:\n",
        "                Q1 = df[column].quantile(0.25)\n",
        "                Q3 = df[column].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - threshold * IQR\n",
        "                upper_bound = Q3 + threshold * IQR\n",
        "                mask = (clean_df[column] >= lower_bound) & (clean_df[column] <= upper_bound)\n",
        "                clean_df = clean_df[mask]\n",
        "        return clean_df\n"
      ],
      "metadata": {
        "id": "ofmP6s4CQ7lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIGHT CURVE PROCESSING"
      ],
      "metadata": {
        "id": "bsV2-Zt8Rxcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LightCurveProcessor:\n",
        "    \"\"\"Process light curves for AstroNet input with global/local views\"\"\"\n",
        "    def __init__(self, global_length=2001, local_length=201):\n",
        "        self.global_length = global_length\n",
        "        self.local_length = local_length\n",
        "\n",
        "    def download_single_lightcurve(self, tic_id):\n",
        "        \"\"\"Download with caching, storing only plain numeric types.\"\"\"\n",
        "        cache_file = f\"lightcurve_{tic_id}.pkl\"\n",
        "        if os.path.exists(cache_file):\n",
        "            try:\n",
        "                lc_data = pd.read_pickle(cache_file)\n",
        "                # Validate loaded data\n",
        "                if not all(key in lc_data for key in ['time', 'flux', 'flux_err']):\n",
        "                    raise ValueError(\"Incomplete light curve data in cache.\")\n",
        "                lc_data['time'] = np.array(lc_data['time'], dtype=float)\n",
        "                lc_data['flux'] = np.array(lc_data['flux'], dtype=float)\n",
        "                lc_data['flux_err'] = np.array(lc_data['flux_err'], dtype=float)\n",
        "                if np.any(~np.isfinite(lc_data['time'])) or np.any(~np.isfinite(lc_data['flux'])):\n",
        "                    raise ValueError(\"Non-finite values in cached light curve.\")\n",
        "                return lc_data\n",
        "            except Exception as e:\n",
        "                print(f\"Cache error for TIC {tic_id}: {e}\")\n",
        "                try:\n",
        "                    os.remove(cache_file)\n",
        "                    print(f\"Removed corrupted cache for TIC {tic_id}\")\n",
        "                except OSError:\n",
        "                    pass\n",
        "\n",
        "        try:\n",
        "            search_result = lk.search_lightcurve(f\"TIC {tic_id}\", mission='TESS', author=['SPOC', 'TESS-SPOC', 'QLP'])\n",
        "            if len(search_result) == 0:\n",
        "                print(f\"No light curves found for TIC {tic_id}\")\n",
        "                return None\n",
        "\n",
        "            lc_collection = search_result.download_all()\n",
        "            if lc_collection is None or len(lc_collection) == 0:\n",
        "                print(f\"No light curves downloaded for TIC {tic_id}\")\n",
        "                return None\n",
        "\n",
        "            # Stitch and basic cleaning\n",
        "            lc = lc_collection.stitch().remove_nans().normalize()\n",
        "\n",
        "            # Convert to plain numeric arrays\n",
        "            time_arr = np.asarray(getattr(lc.time, 'jd', lc.time)).astype(float)\n",
        "            flux = lc.flux\n",
        "            try:\n",
        "                flux_arr = np.asarray(flux.value).astype(float)\n",
        "            except Exception:\n",
        "                try:\n",
        "                    flux_arr = np.asarray(flux.filled(np.nan)).astype(float)\n",
        "                except Exception:\n",
        "                    flux_arr = np.asarray(flux).astype(float)\n",
        "\n",
        "            flux_err = getattr(lc, 'flux_err', None)\n",
        "            if flux_err is None:\n",
        "                flux_err_arr = np.full_like(flux_arr, np.nan)\n",
        "            else:\n",
        "                try:\n",
        "                    flux_err_arr = np.asarray(flux_err.value).astype(float)\n",
        "                except Exception:\n",
        "                    try:\n",
        "                        flux_err_arr = np.asarray(flux_err.filled(np.nan)).astype(float)\n",
        "                    except Exception:\n",
        "                        flux_err_arr = np.asarray(flux_err).astype(float)\n",
        "\n",
        "            # Validate data\n",
        "            if np.any(~np.isfinite(time_arr)) or np.any(~np.isfinite(flux_arr)):\n",
        "                print(f\"Non-finite values detected in light curve for TIC {tic_id}\")\n",
        "                return None\n",
        "\n",
        "            # Prepare plain-datatype dict for pickling\n",
        "            lc_data = {\n",
        "                'tic_id': int(tic_id) if tic_id is not None else tic_id,\n",
        "                'time': time_arr.tolist(),\n",
        "                'flux': flux_arr.tolist(),\n",
        "                'flux_err': flux_err_arr.tolist()\n",
        "            }\n",
        "\n",
        "            # Save cache\n",
        "            with open(cache_file, 'wb') as f:\n",
        "                pickle.dump(lc_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "            # Convert back to numpy arrays for downstream use\n",
        "            lc_data['time'] = np.array(lc_data['time'], dtype=float)\n",
        "            lc_data['flux'] = np.array(lc_data['flux'], dtype=float)\n",
        "            lc_data['flux_err'] = np.array(lc_data['flux_err'], dtype=float)\n",
        "\n",
        "            return lc_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Download error for TIC {tic_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def phase_fold_lightcurve(self, time, flux, period, t0=0):\n",
        "        \"\"\"Phase-fold to center transit\"\"\"\n",
        "        if period <= 0 or not np.isfinite(period):\n",
        "            return time, flux\n",
        "        phase = ((time - t0) % period) / period\n",
        "        phase = phase - np.floor(phase + 0.5)  # Center at 0\n",
        "        sorted_idx = np.argsort(phase)\n",
        "        return phase[sorted_idx], flux[sorted_idx]\n",
        "\n",
        "    def get_local_view(self, phase, flux, period, transit_center=0, zoom_width=4):\n",
        "        \"\"\"Local view around transit (hours, assuming cadence)\"\"\"\n",
        "        period_hours = period * 24  # Convert period to hours\n",
        "        phase_width = zoom_width / period_hours if period_hours > 0 else 0.1\n",
        "        mask = np.abs(phase - transit_center) < phase_width\n",
        "        if not np.any(mask):\n",
        "            print(\"No points in local view; returning full light curve.\")\n",
        "            return phase, flux\n",
        "        return phase[mask], flux[mask]\n",
        "\n",
        "    def bin_lightcurve(self, phase, flux, num_bins):\n",
        "        \"\"\"Bin phase-folded LC\"\"\"\n",
        "        bins = np.linspace(-0.5, 0.5, num_bins + 1)\n",
        "        binned_flux, _, _ = stats.binned_statistic(phase, flux, statistic='mean', bins=bins)\n",
        "        binned_flux = np.nan_to_num(binned_flux)\n",
        "        return binned_flux\n",
        "\n",
        "    def preprocess_lightcurve(self, time, flux, period=None, t0=0, detrend=True, normalize=True, augment=False):\n",
        "        \"\"\"Preprocess with phase-folding, global/local, binning\"\"\"\n",
        "        # Ensure inputs are NumPy arrays\n",
        "        time = np.asarray(time, dtype=float)\n",
        "        flux = np.asarray(flux, dtype=float)\n",
        "\n",
        "        # Create mask for finite values\n",
        "        mask = np.isfinite(time) & np.isfinite(flux)\n",
        "        if not np.any(mask):\n",
        "            print(\"No valid data points after filtering non-finite values.\")\n",
        "            return None, None\n",
        "\n",
        "        time, flux = time[mask], flux[mask]\n",
        "        if len(flux) < 100:\n",
        "            print(\"Insufficient data points after filtering (< 100).\")\n",
        "            return None, None\n",
        "\n",
        "        # Normalize\n",
        "        if normalize:\n",
        "            median_flux = np.median(flux)\n",
        "            if median_flux == 0 or not np.isfinite(median_flux):\n",
        "                print(\"Invalid median flux for normalization.\")\n",
        "                return None, None\n",
        "            flux = flux / median_flux\n",
        "\n",
        "        # Detrend\n",
        "        if detrend:\n",
        "            try:\n",
        "                lc = lk.LightCurve(time=time, flux=flux)\n",
        "                flat_lc = lc.flatten(window_length=101)\n",
        "                flux = flat_lc.flux\n",
        "            except Exception as e:\n",
        "                print(f\"Detrending failed: {e}\")\n",
        "                return None, None\n",
        "\n",
        "        # Augment\n",
        "        if augment:\n",
        "            flux += np.random.normal(0, 0.01 * np.std(flux), len(flux))\n",
        "\n",
        "        # Phase-fold\n",
        "        if period is not None and period > 0 and np.isfinite(period):\n",
        "            phase, flux = self.phase_fold_lightcurve(time, flux, period, t0)\n",
        "            global_flux = self.bin_lightcurve(phase, flux, self.global_length)\n",
        "            local_phase, local_flux = self.get_local_view(phase, flux, period=period)\n",
        "            local_flux = self.bin_lightcurve(local_phase, local_flux, self.local_length)\n",
        "        else:\n",
        "            global_flux = self.resample_to_fixed_length(flux, self.global_length)\n",
        "            local_flux = global_flux  # Fallback\n",
        "\n",
        "        return global_flux, local_flux\n",
        "\n",
        "    def resample_to_fixed_length(self, data, length):\n",
        "        \"\"\"Resample data to a fixed length\"\"\"\n",
        "        if len(data) == length:\n",
        "            return data\n",
        "        x_old = np.linspace(0, 1, len(data))\n",
        "        x_new = np.linspace(0, 1, length)\n",
        "        interp = interp1d(x_old, data, kind='linear', fill_value='extrapolate')\n",
        "        return interp(x_new)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFR7MTu-q7FQ",
        "outputId": "e8e4cfea-8d10-44ed-d30e-c1fc4e54a405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightkurve\n",
            "  Downloading lightkurve-2.5.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: astropy>=5.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (7.1.0)\n",
            "Collecting astroquery>=0.3.10 (from lightkurve)\n",
            "  Downloading astroquery-0.4.11-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (4.13.5)\n",
            "Requirement already satisfied: bokeh>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (3.7.3)\n",
            "Collecting fbpca>=1.0 (from lightkurve)\n",
            "  Downloading fbpca-1.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.1 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (3.10.0)\n",
            "Collecting memoization>=0.3.1 (from lightkurve)\n",
            "  Downloading memoization-0.4.0.tar.gz (41 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.3.6 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (1.0.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (2.32.4)\n",
            "Collecting s3fs>=2024.6.1 (from lightkurve)\n",
            "  Downloading s3fs-2025.9.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (1.16.2)\n",
            "Requirement already satisfied: tqdm>=4.25.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (4.67.1)\n",
            "Collecting uncertainties>=3.1.4 (from lightkurve)\n",
            "  Downloading uncertainties-3.2.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: urllib3>=1.23 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (2.5.0)\n",
            "Requirement already satisfied: pyerfa>=2.0.1.1 in /usr/local/lib/python3.12/dist-packages (from astropy>=5.0->lightkurve) (2.0.1.5)\n",
            "Requirement already satisfied: astropy-iers-data>=0.2025.4.28.0.37.27 in /usr/local/lib/python3.12/dist-packages (from astropy>=5.0->lightkurve) (0.2025.9.29.0.35.48)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from astropy>=5.0->lightkurve) (6.0.3)\n",
            "Requirement already satisfied: packaging>=22.0.0 in /usr/local/lib/python3.12/dist-packages (from astropy>=5.0->lightkurve) (25.0)\n",
            "Requirement already satisfied: html5lib>=0.999 in /usr/local/lib/python3.12/dist-packages (from astroquery>=0.3.10->lightkurve) (1.1)\n",
            "Requirement already satisfied: keyring>=15.0 in /usr/local/lib/python3.12/dist-packages (from astroquery>=0.3.10->lightkurve) (25.6.0)\n",
            "Collecting pyvo>=1.5 (from astroquery>=0.3.10->lightkurve)\n",
            "  Downloading pyvo-1.7-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.6.0->lightkurve) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.6.0->lightkurve) (4.15.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (3.1.6)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (1.3.3)\n",
            "Requirement already satisfied: narwhals>=1.13 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (2.6.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (11.3.0)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (2025.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.6->lightkurve) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.6->lightkurve) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->lightkurve) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->lightkurve) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->lightkurve) (2025.8.3)\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading aiobotocore-2.24.2-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting fsspec==2025.9.0 (from s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from s3fs>=2024.6.1->lightkurve) (3.12.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->lightkurve) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->lightkurve) (3.6.0)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting botocore<1.40.19,>=1.40.15 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading botocore-1.40.18-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve) (6.6.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (1.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (1.20.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.12/dist-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=2.9->bokeh>=2.3.2->lightkurve) (3.0.3)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (3.4.0)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (0.9.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (3.4.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (4.3.0)\n",
            "Requirement already satisfied: jaraco.context in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (6.0.1)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.12/dist-packages (from SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (43.0.3)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from jaraco.classes->keyring>=15.0->astroquery>=0.3.10->lightkurve) (10.8.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (2.23)\n",
            "Downloading lightkurve-2.5.1-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astroquery-0.4.11-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3fs-2025.9.0-py3-none-any.whl (30 kB)\n",
            "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uncertainties-3.2.3-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiobotocore-2.24.2-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyvo-1.7-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading botocore-1.40.18-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: fbpca, memoization\n",
            "  Building wheel for fbpca (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fbpca: filename=fbpca-1.0-py3-none-any.whl size=11373 sha256=52314ea78eda7cc2bafc49da511d3abebaf7d16fbed5b8d9dcb039850c1cce27\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/15/cd/2f622795b09e83471a3be5d2581cd9cf96a6ec7aa78e8deffe\n",
            "  Building wheel for memoization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memoization: filename=memoization-0.4.0-py3-none-any.whl size=50452 sha256=9a8580997e5ccf00a4a69677c6d10a5b3379baa63eac24d73ab85907d4394253\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/35/02/90618fc7cbf03a335f3cacd59d32b35930bf5a57f3c0d0814c\n",
            "Successfully built fbpca memoization\n",
            "Installing collected packages: fbpca, uncertainties, memoization, jmespath, fsspec, aioitertools, botocore, pyvo, aiobotocore, s3fs, astroquery, lightkurve\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiobotocore-2.24.2 aioitertools-0.12.0 astroquery-0.4.11 botocore-1.40.18 fbpca-1.0 fsspec-2025.9.0 jmespath-1.0.1 lightkurve-2.5.1 memoization-0.4.0 pyvo-1.7 s3fs-2025.9.0 uncertainties-3.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transfer Learning"
      ],
      "metadata": {
        "id": "SZy6y-sOSS-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExoplanetTransferLearningModel:\n",
        "    \"\"\"Improved AstroNet-like model\"\"\"\n",
        "    def __init__(self, global_shape=(2001, 1), local_shape=(201, 1), stellar_shape=(3,), num_classes=1):\n",
        "        self.global_shape = global_shape\n",
        "        self.local_shape = local_shape\n",
        "        self.stellar_shape = stellar_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = None\n",
        "\n",
        "    def create_model(self):\n",
        "        global_input = layers.Input(shape=self.global_shape, name='global_lc')\n",
        "        local_input = layers.Input(shape=self.local_shape, name='local_lc')\n",
        "        stellar_input = layers.Input(shape=self.stellar_shape, name='stellar_params')\n",
        "\n",
        "        def cnn_branch(input_layer, filters=[16, 32, 64, 128, 256]):\n",
        "            x = input_layer\n",
        "            for f in filters:\n",
        "                x = layers.Conv1D(f, 5, activation='relu', padding='same')(x)\n",
        "                x = layers.MaxPooling1D(5, strides=2, padding='same')(x)\n",
        "            x = layers.Flatten()(x)\n",
        "            return x\n",
        "\n",
        "        x_global = cnn_branch(global_input)\n",
        "        x_local = cnn_branch(local_input)\n",
        "\n",
        "        x = layers.Concatenate()([x_global, x_local])\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        residual = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Add()([x, residual])\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "\n",
        "        s = layers.Dense(32, activation='relu')(stellar_input)\n",
        "        x = layers.Concatenate()([x, s])\n",
        "        x = layers.Dense(128, activation='relu')(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "        self.model = keras.Model(inputs=[global_input, local_input, stellar_input], outputs=outputs)\n",
        "        return self.model\n",
        "\n",
        "    def compile_model(self, learning_rate=0.001):\n",
        "        self.model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate),\n",
        "            loss=BinaryFocalCrossentropy(alpha=0.25, gamma=2.0),\n",
        "            metrics=['accuracy', keras.metrics.Precision(name='precision'), keras.metrics.Recall(name='recall'), keras.metrics.AUC(name='auc')]\n",
        "        )\n",
        "        print(\"âœ… Model compiled with focal loss\")\n",
        "\n",
        "    def setup_callbacks(self):\n",
        "        return [\n",
        "            callbacks.EarlyStopping(monitor='val_auc', patience=20, mode='max', restore_best_weights=True),\n",
        "            callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=10, min_lr=1e-6, mode='max'),\n",
        "            callbacks.ModelCheckpoint('best_model.h5', monitor='val_auc', save_best_only=True, mode='max')\n",
        "        ]"
      ],
      "metadata": {
        "id": "GXukFD0wSYKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Traning and Evaluation"
      ],
      "metadata": {
        "id": "q3BfsKdrSe3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTrainer:\n",
        "    \"\"\"Train and evaluate the exoplanet detection model\"\"\"\n",
        "    def __init__(self):\n",
        "        self.history = None\n",
        "\n",
        "    def prepare_lc_datasets(self, datasets, cleaned_index, processor, preprocessor):\n",
        "        global_lc = []\n",
        "        local_lc = []\n",
        "        stellar = []\n",
        "        labels = []\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "        for idx in cleaned_index:\n",
        "            tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "            period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "            label = exominer_data.loc[idx, 'label']\n",
        "            lc_data = processor.download_single_lightcurve(tic_id)\n",
        "            if lc_data is not None:\n",
        "                g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "                if g_flux is not None and l_flux is not None:\n",
        "                    global_lc.append(g_flux)\n",
        "                    local_lc.append(l_flux)\n",
        "                    stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]', 'Stellar Radius [Solar Radii]', 'Stellar Surface Gravity [log10(cm/s**2)]']].values\n",
        "                    stellar_row = np.nan_to_num(stellar_row)\n",
        "                    # Scale stellar features using the stellar scaler\n",
        "                    try:\n",
        "                        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "                            scaler = pickle.load(f)\n",
        "                        stellar_row = scaler.transform(stellar_row.reshape(1, -1)).flatten()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading stellar scaler for TIC {tic_id}: {e}. Using raw stellar features.\")\n",
        "                        stellar_row = stellar_row  # Fallback to raw features\n",
        "                    stellar.append(stellar_row)\n",
        "                    labels.append(label)\n",
        "        if not global_lc:\n",
        "            raise ValueError(\"No valid light curves processed.\")\n",
        "        global_lc = np.array(global_lc).reshape(-1, processor.global_length, 1)\n",
        "        local_lc = np.array(local_lc).reshape(-1, processor.local_length, 1)\n",
        "        stellar = np.array(stellar)\n",
        "        labels = np.array(labels)\n",
        "        X_train_g, X_test_g, X_train_l, X_test_l, X_train_s, X_test_s, y_train, y_test = train_test_split(\n",
        "            global_lc, local_lc, stellar, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "        )\n",
        "        X_train_g, X_val_g, X_train_l, X_val_l, X_train_s, X_val_s, y_train, y_val = train_test_split(\n",
        "            X_train_g, X_train_l, X_train_s, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        "        )\n",
        "        return X_train_g, X_val_g, X_test_g, X_train_l, X_val_l, X_test_l, X_train_s, X_val_s, X_test_s, y_train, y_val, y_test\n",
        "\n",
        "    def compute_class_weights(self, y_train):\n",
        "        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "        weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "        print(f\"ğŸ“Š Class weights: {weight_dict}\")\n",
        "        return weight_dict\n",
        "\n",
        "    def train_model(self, model, X_train_g, X_train_l, X_train_s, y_train, X_val_g, X_val_l, X_val_s, y_val, epochs=200, batch_size=64, class_weight=None):\n",
        "        callbacks = model.setup_callbacks()\n",
        "        self.history = model.model.fit(\n",
        "            [X_train_g, X_train_l, X_train_s], y_train,\n",
        "            validation_data=([X_val_g, X_val_l, X_val_s], y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            class_weight=class_weight,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "        return self.history\n",
        "\n",
        "    def evaluate_model(self, model, X_test_g, X_test_l, X_test_s, y_test):\n",
        "        model.model = keras.models.load_model('best_model.h5')\n",
        "        y_pred_proba = model.model.predict([X_test_g, X_test_l, X_test_s], verbose=0)\n",
        "        y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "        test_accuracy = accuracy_score(y_test, y_pred)\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        print(f\"\\nğŸ“ˆ TEST RESULTS:\")\n",
        "        print(f\"  Accuracy:  {test_accuracy:.4f}\")\n",
        "        print(f\"  ROC AUC:   {roc_auc:.4f}\")\n",
        "        print(f\"  Precision: {report['1']['precision']:.4f}\")\n",
        "        print(f\"  Recall:    {report['1']['recall']:.4f}\")\n",
        "        return y_pred, y_pred_proba\n",
        "\n",
        "    def cross_validate(self, model_class, datasets, cleaned_index, processor, preprocessor, n_splits=5):\n",
        "        auc_scores = []\n",
        "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "        labels = datasets['ExoMiner']['label'].loc[cleaned_index]\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
        "            print(f\"Cross-validation Fold {fold+1}/{n_splits}\")\n",
        "            train_cleaned = cleaned_index[train_idx]\n",
        "            val_cleaned = cleaned_index[val_idx]\n",
        "            X_train_g, X_val_g, X_train_l, X_val_l, X_train_s, X_val_s, y_train, y_val = self.prepare_lc_datasets_for_cv(datasets, train_cleaned, val_cleaned, processor, preprocessor)\n",
        "            model = model_class()\n",
        "            model.create_model()\n",
        "            model.compile_model()\n",
        "            self.train_model(model, X_train_g, X_train_l, X_train_s, y_train, X_val_g, X_val_l, X_val_s, y_val, epochs=50)\n",
        "            y_pred_proba = model.model.predict([X_val_g, X_val_l, X_val_s], verbose=0)\n",
        "            fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
        "            auc_scores.append(auc(fpr, tpr))\n",
        "        print(f\"\\nCross-validation AUC: {np.mean(auc_scores):.3f} Â± {np.std(auc_scores):.3f}\")\n",
        "        return auc_scores\n",
        "\n",
        "    def prepare_lc_datasets_for_cv(self, datasets, train_index, val_index, processor, preprocessor):\n",
        "        global_lc_train = []\n",
        "        local_lc_train = []\n",
        "        stellar_train = []\n",
        "        labels_train = []\n",
        "        global_lc_val = []\n",
        "        local_lc_val = []\n",
        "        stellar_val = []\n",
        "        labels_val = []\n",
        "\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "\n",
        "        # Process training data\n",
        "        for idx in train_index:\n",
        "            tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "            period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "            label = exominer_data.loc[idx, 'label']\n",
        "            lc_data = processor.download_single_lightcurve(tic_id)\n",
        "            if lc_data is not None:\n",
        "                g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "                if g_flux is not None and l_flux is not None:\n",
        "                    global_lc_train.append(g_flux)\n",
        "                    local_lc_train.append(l_flux)\n",
        "                    stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]', 'Stellar Radius [Solar Radii]', 'Stellar Surface Gravity [log10(cm/s**2)]']].values\n",
        "                    stellar_row = np.nan_to_num(stellar_row)\n",
        "                    try:\n",
        "                        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "                            scaler = pickle.load(f)\n",
        "                        stellar_row = scaler.transform(stellar_row.reshape(1, -1)).flatten()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading stellar scaler for TIC {tic_id}: {e}. Using raw stellar features.\")\n",
        "                        stellar_row = stellar_row\n",
        "                    stellar_train.append(stellar_row)\n",
        "                    labels_train.append(label)\n",
        "\n",
        "        # Process validation data\n",
        "        for idx in val_index:\n",
        "            tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "            period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "            label = exominer_data.loc[idx, 'label']\n",
        "            lc_data = processor.download_single_lightcurve(tic_id)\n",
        "            if lc_data is not None:\n",
        "                g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "                if g_flux is not None and l_flux is not None:\n",
        "                    global_lc_val.append(g_flux)\n",
        "                    local_lc_val.append(l_flux)\n",
        "                    stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]', 'Stellar Radius [Solar Radii]', 'Stellar Surface Gravity [log10(cm/s**2)]']].values\n",
        "                    stellar_row = np.nan_to_num(stellar_row)\n",
        "                    try:\n",
        "                        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "                            scaler = pickle.load(f)\n",
        "                        stellar_row = scaler.transform(stellar_row.reshape(1, -1)).flatten()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading stellar scaler for TIC {tic_id}: {e}. Using raw stellar features.\")\n",
        "                        stellar_row = stellar_row\n",
        "                    stellar_val.append(stellar_row)\n",
        "                    labels_val.append(label)\n",
        "\n",
        "        if not global_lc_train or not global_lc_val:\n",
        "            raise ValueError(\"No valid light curves processed for cross-validation.\")\n",
        "\n",
        "        X_train_g = np.array(global_lc_train).reshape(-1, processor.global_length, 1)\n",
        "        X_train_l = np.array(local_lc_train).reshape(-1, processor.local_length, 1)\n",
        "        X_train_s = np.array(stellar_train)\n",
        "        y_train = np.array(labels_train)\n",
        "\n",
        "        X_val_g = np.array(global_lc_val).reshape(-1, processor.global_length, 1)\n",
        "        X_val_l = np.array(local_lc_val).reshape(-1, processor.local_length, 1)\n",
        "        X_val_s = np.array(stellar_val)\n",
        "        y_val = np.array(labels_val)\n",
        "\n",
        "        return X_train_g, X_val_g, X_train_l, X_val_l, X_train_s, X_val_s, y_train, y_val\n"
      ],
      "metadata": {
        "id": "vJY6doSLSry6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyper Parameter Tuning"
      ],
      "metadata": {
        "id": "uEKXyqraStVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_hyperparameters(X_train_g, X_train_l, X_train_s, y_train, X_val_g, X_val_l, X_val_s, y_val):\n",
        "    \"\"\"Tune hyperparameters using Keras Tuner\"\"\"\n",
        "    def build_model(hp):\n",
        "        model = ExoplanetTransferLearningModel()\n",
        "        model.create_model()\n",
        "        model.compile_model(learning_rate=hp.Float('lr', 1e-5, 1e-3, sampling='log'))\n",
        "        return model.model\n",
        "\n",
        "    tuner = kt.Hyperband(\n",
        "        build_model,\n",
        "        objective='val_auc',\n",
        "        max_epochs=200,\n",
        "        factor=3,\n",
        "        directory='tuner_dir',\n",
        "        project_name='exoplanet'\n",
        "    )\n",
        "    tuner.search([X_train_g, X_train_l, X_train_s], y_train,\n",
        "                 validation_data=([X_val_g, X_val_l, X_val_s], y_val),\n",
        "                 epochs=50)\n",
        "    best_hps = tuner.get_best_hyperparameters()[0]\n",
        "    print(f\"Best learning rate: {best_hps.get('lr')}\")\n",
        "    return tuner.get_best_models(1)[0], best_hps\n",
        "\n",
        "# ============================================\n",
        "# 7. VISUALIZATION AND ANALYSIS\n",
        "# ============================================\n",
        "\n",
        "def visualize_results(history, y_test, y_pred, y_pred_proba):\n",
        "    \"\"\"Visualize training history and evaluation metrics\"\"\"\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history.history['accuracy'], label='accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "    plt.plot(history.history['auc'], label='auc')\n",
        "    plt.plot(history.history['val_auc'], label='val_auc')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Metric Value')\n",
        "    plt.title('Training History')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot ROC curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=['Not Exoplanet', 'Exoplanet'],\n",
        "                yticklabels=['Not Exoplanet', 'Exoplanet'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "def test_with_real_lightcurves(model, datasets, preprocessor, num_samples=10):\n",
        "    \"\"\"Test the model with real light curves not used in training\"\"\"\n",
        "    print(f\"\\nğŸ”¬ Testing with {num_samples} real light curves:\")\n",
        "    exominer_data = datasets['ExoMiner']\n",
        "    all_indices = exominer_data.index\n",
        "    cleaned_index = datasets['ExoMiner_Features'].index\n",
        "    unseen_indices = all_indices.difference(cleaned_index)\n",
        "    if len(unseen_indices) < num_samples:\n",
        "        print(\"Not enough unseen samples to test.\")\n",
        "        return\n",
        "\n",
        "    test_indices = np.random.choice(unseen_indices, num_samples, replace=False)\n",
        "    processor = LightCurveProcessor()\n",
        "    try:\n",
        "        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "            scaler = pickle.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading stellar scaler: {e}. Using raw stellar features.\")\n",
        "        scaler = None\n",
        "\n",
        "    for idx in test_indices:\n",
        "        tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "        period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "        true_label = exominer_data.loc[idx, 'label']\n",
        "        lc_data = processor.download_single_lightcurve(tic_id)\n",
        "        if lc_data is not None:\n",
        "            g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "            if g_flux is not None and l_flux is not None:\n",
        "                stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]', 'Stellar Radius [Solar Radii]', 'Stellar Surface Gravity [log10(cm/s**2)]']].values\n",
        "                stellar_row = np.nan_to_num(stellar_row).reshape(1, -1)\n",
        "                if scaler is not None:\n",
        "                    stellar_row = scaler.transform(stellar_row).flatten()\n",
        "                g_flux = np.array(g_flux).reshape(1, processor.global_length, 1)\n",
        "                l_flux = np.array(l_flux).reshape(1, processor.local_length, 1)\n",
        "                y_pred_proba = model.model.predict([g_flux, l_flux, stellar_row], verbose=0)\n",
        "                predicted_label = (y_pred_proba > 0.5).astype(int).flatten()[0]\n",
        "                print(f\"  TIC ID: {tic_id}, True Label: {true_label}, Predicted Probability: {y_pred_proba[0][0]:.4f}, Predicted Label: {predicted_label}\")\n",
        "            else:\n",
        "                print(f\"  TIC ID: {tic_id} - Preprocessing failed.\")\n",
        "        else:\n",
        "            print(f\"  TIC ID: {tic_id} - Light curve download failed.\")\n",
        "\n",
        "def analyze_feature_importance(datasets):\n",
        "    \"\"\"Analyze feature importance using RandomForest\"\"\"\n",
        "    print(\"\\nğŸ“Š Analyzing Feature Importance:\")\n",
        "    exominer_features = datasets['ExoMiner_Features']\n",
        "    labels = datasets['ExoMiner'].loc[exominer_features.index, 'label']\n",
        "\n",
        "    if len(exominer_features) == 0 or len(labels) == 0:\n",
        "        print(\"Not enough data to analyze feature importance.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(exominer_features, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        rf_model.fit(X_train, y_train)\n",
        "        importances = rf_model.feature_importances_\n",
        "        feature_names = exominer_features.columns\n",
        "        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "        feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
        "        print(\"Top 10 Feature Importance:\")\n",
        "        print(feature_importance_df.head(10))\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x='importance', y='feature', data=feature_importance_df.head(10), palette='viridis')\n",
        "        plt.title('Top 10 Feature Importance from RandomForest')\n",
        "        plt.xlabel('Importance')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during feature importance analysis: {e}\")\n"
      ],
      "metadata": {
        "id": "QdYxJZUESrtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Execution of pipeline"
      ],
      "metadata": {
        "id": "Tk0N33K_S53x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL0yDqFDhdev",
        "outputId": "237f0349-3b16-486a-b6ce-b2f652a8693f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightkurve/prf/__init__.py:7: UserWarning: Warning: the tpfmodel submodule is not available without oktopus installed, which requires a current version of autograd. See #1452 for details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Libraries imported successfully\n",
            "TensorFlow version: 2.19.0\n",
            "Lightkurve version: 2.5.1\n",
            "Loading K2 dataset...\n",
            "Loading ExoMiner dataset...\n",
            "Loading TOI dataset...\n",
            "Merging ExoMiner with TOI...\n",
            "Loading Cumulative dataset...\n",
            "âœ… Labels created\n",
            "ExoMiner - Planets: 7126, Non-planets: 5641\n",
            "âœ… Features engineered\n",
            "Original samples: 12767, After cleaning: 8226\n",
            "Feature columns: ['Orbital Period [day]', 'Transit Duration [hour]', 'Transit Depth [ppm]', 'Planet Radius [Earth Radii]', 'MES', 'Transit Model SNR', 'Number of transits observed', 'Stellar Effective Temperature [K]', 'Stellar Radius [Solar Radii]', 'Stellar Surface Gravity [log10(cm/s**2)]', 'depth_duration_ratio', 'period_snr_ratio']\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    \"\"\"Execute the exoplanet detection pipeline\"\"\"\n",
        "    datasets = load_and_parse_datasets()\n",
        "    preprocessor = ExoplanetDataPreprocessor()\n",
        "    datasets = preprocessor.create_labels(datasets)\n",
        "    datasets, cleaned_index = preprocessor.engineer_features(datasets)\n",
        "    processor = LightCurveProcessor()\n",
        "    trainer = ModelTrainer()\n",
        "    X_train_g, X_val_g, X_test_g, X_train_l, X_val_l, X_test_l, X_train_s, X_val_s, X_test_s, y_train, y_val, y_test = trainer.prepare_lc_datasets(datasets, cleaned_index, processor, preprocessor)\n",
        "    best_model_hp, best_hps = tune_hyperparameters(X_train_g, X_train_l, X_train_s, y_train, X_val_g, X_val_l, X_val_s, y_val)\n",
        "    # Re-create and train the final model\n",
        "    model = ExoplanetTransferLearningModel()\n",
        "    model.create_model()\n",
        "    model.compile_model(learning_rate=best_hps.get('lr'))\n",
        "    X_train_g_full = np.concatenate((X_train_g, X_val_g))\n",
        "    X_train_l_full = np.concatenate((X_train_l, X_val_l))\n",
        "    X_train_s_full = np.concatenate((X_train_s, X_val_s))\n",
        "    y_train_full = np.concatenate((y_train, y_val))\n",
        "    class_weights = trainer.compute_class_weights(y_train_full)\n",
        "    history = trainer.train_model(model, X_train_g_full, X_train_l_full, X_train_s_full, y_train_full, X_test_g, X_test_l, X_test_s, y_test, epochs=200, batch_size=64, class_weight=class_weights)\n",
        "    y_pred, y_pred_proba = trainer.evaluate_model(model, X_test_g, X_test_l, X_test_s, y_test)\n",
        "    visualize_results(history, y_test, y_pred, y_pred_proba)\n",
        "    test_with_real_lightcurves(model, datasets, preprocessor, num_samples=10)\n",
        "    analyze_feature_importance(datasets)\n",
        "    auc_scores = trainer.cross_validate(ExoplanetTransferLearningModel, datasets, cleaned_index, processor, preprocessor)\n",
        "    return model, history\n",
        "\n",
        "# Run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        model, history = main()\n",
        "        print(\"\\nğŸ‰ PIPELINE COMPLETED!\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zEBkuSH3jkvr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "965986f5-014f-418f-a7dc-664b0b35c948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (from keras_tuner) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras_tuner) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from keras_tuner) (2.32.4)\n",
            "Collecting kt-legacy (from keras_tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (0.5.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras->keras_tuner) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras_tuner) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras_tuner) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras_tuner\n",
            "Successfully installed keras_tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XGz3YDWgjltJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ebe798-5729-494c-8575-2417fcdb4a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wn0yVJUCuZat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZYxJiCSCudg8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}