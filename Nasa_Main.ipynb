{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khaled-Wael/NASA_Project/blob/YU/Nasa_Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXOPLANET DETECTION PIPELINE\n",
        "!pip install lightkurve\n",
        "!pip install tensorflow\n",
        "!pip install keras_tuner\n",
        "\n",
        "#Libraries Upload\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as pltn\n",
        "import seaborn as sns\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy import stats\n",
        "import lightkurve as lk\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, auc\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
        "import keras_tuner as kt\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"✅ Libraries imported successfully\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Lightkurve version: {lk.__version__}\")\n",
        "\n",
        "# Data Loading\n",
        "# 1. DATA LOADING AND PARSING\n",
        "# ============================================\n",
        "def load_and_parse_datasets():\n",
        "    \"\"\"Load and merge datasets for stellar params\"\"\"\n",
        "    datasets = {}\n",
        "\n",
        "    # Load K2\n",
        "    print(\"Loading K2 dataset...\")\n",
        "    with open('/content/sample_data/k2pandc_2025.09.25_19.49.42.csv', 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        header_line = None\n",
        "        for i, line in enumerate(lines):\n",
        "            if 'pl_name' in line and 'hostname' in line:\n",
        "                header_line = i\n",
        "                break\n",
        "        if header_line:\n",
        "            k2_data = pd.read_csv('/content/sample_data/k2pandc_2025.09.25_19.49.42.csv', skiprows=header_line, sep=',', engine='python')\n",
        "        else:\n",
        "            k2_data = pd.read_csv('/content/sample_data/k2pandc_2025.09.25_19.49.42.csv', skiprows=97, header=None, names=['raw_data'])\n",
        "    datasets['K2'] = k2_data\n",
        "\n",
        "    # Load ExoMiner\n",
        "    print(\"Loading ExoMiner dataset...\")\n",
        "    exominer_data = pd.read_csv('/content/sample_data/exominer_vetting_tess-spoc-2-min-s1s67_dashtable_dvm-url_scoregt0.1 (1).csv')\n",
        "    datasets['ExoMiner'] = exominer_data\n",
        "\n",
        "    # Load TOI\n",
        "    print(\"Loading TOI dataset...\")\n",
        "    try:\n",
        "        toi_data = pd.read_csv('/content/sample_data/TOI_2025.09.25_19.49.24.csv', comment='#', engine='python')\n",
        "        datasets['TOI'] = toi_data\n",
        "    except:\n",
        "        datasets['TOI'] = pd.DataFrame()\n",
        "\n",
        "    # Merge ExoMiner with TOI\n",
        "    if not datasets['TOI'].empty:\n",
        "        print(\"Merging ExoMiner with TOI...\")\n",
        "        exominer_data = exominer_data.merge(toi_data, left_on='TIC ID', right_on='tid', how='left')\n",
        "        exominer_data = exominer_data.rename(columns={\n",
        "            'st_teff': 'Stellar Effective Temperature [K]',\n",
        "            'st_rad': 'Stellar Radius [Solar Radii]',\n",
        "            'st_logg': 'Stellar Surface Gravity [log10(cm/s*2)]'\n",
        "        })\n",
        "        datasets['ExoMiner'] = exominer_data\n",
        "\n",
        "    # Load Cumulative\n",
        "    print(\"Loading Cumulative dataset...\")\n",
        "    try:\n",
        "        cumulative_data = pd.read_csv('/content/sample_data/cumulative_2025.09.25_19.49.05.csv', comment='#', engine='python')\n",
        "        datasets['Cumulative'] = cumulative_data\n",
        "    except:\n",
        "        datasets['Cumulative'] = pd.DataFrame()\n",
        "\n",
        "    return datasets\n",
        "\n",
        "# Data Preprocessing and Feature Engineering\n",
        "class ExoplanetDataPreprocessor:\n",
        "    \"\"\"Preprocess exoplanet data for machine learning\"\"\"\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()  # For all features\n",
        "        self.stellar_scaler = StandardScaler()  # For stellar features only\n",
        "        self.feature_columns = []\n",
        "        self.scaler_file = 'scaler.pkl'\n",
        "        self.stellar_scaler_file = 'stellar_scaler.pkl'\n",
        "\n",
        "    def create_labels(self, datasets):\n",
        "        \"\"\"Create binary labels for classification\"\"\"\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "        exominer_data['label'] = (exominer_data['ExoMiner Score'] > 0.7).astype(int)\n",
        "        print(\"✅ Labels created\")\n",
        "        print(f\"ExoMiner - Planets: {exominer_data['label'].sum()}, \"\n",
        "              f\"Non-planets: {len(exominer_data) - exominer_data['label'].sum()}\")\n",
        "        return datasets\n",
        "\n",
        "    def engineer_features(self, datasets):\n",
        "        \"\"\"Create features from available data\"\"\"\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "\n",
        "        # Define feature columns\n",
        "        transit_columns = [\n",
        "            'Orbital Period [day]',\n",
        "            'Transit Duration [hour]',\n",
        "            'Transit Depth [ppm]',\n",
        "            'Planet Radius [Earth Radii]',\n",
        "            'MES',\n",
        "            'Transit Model SNR',\n",
        "            'Number of transits observed'\n",
        "        ]\n",
        "        stellar_columns = [\n",
        "            'Stellar Effective Temperature [K]',\n",
        "            'Stellar Radius [Solar Radii]',\n",
        "            'Stellar Surface Gravity [log10(cm/s*2)]'\n",
        "        ]\n",
        "        feature_columns = transit_columns + stellar_columns\n",
        "        features = exominer_data[feature_columns].copy()\n",
        "\n",
        "        # Impute missing\n",
        "        features = features.fillna(features.mean())\n",
        "\n",
        "        # Store original index\n",
        "        original_index = features.index\n",
        "\n",
        "        # Remove outliers\n",
        "        features_cleaned = self.remove_outliers_iqr(features, threshold=2.0)\n",
        "\n",
        "        # Cleaned index\n",
        "        cleaned_index = features_cleaned.index\n",
        "\n",
        "        # Derived features\n",
        "        features_cleaned['depth_duration_ratio'] = features_cleaned['Transit Depth [ppm]'] / features_cleaned['Transit Duration [hour]']\n",
        "        features_cleaned['period_snr_ratio'] = features_cleaned['Orbital Period [day]'] / features_cleaned['Transit Model SNR']\n",
        "\n",
        "        # Scale stellar features separately\n",
        "        stellar_features = features_cleaned[stellar_columns]\n",
        "        stellar_scaled = self.stellar_scaler.fit_transform(stellar_features)\n",
        "        with open(self.stellar_scaler_file, 'wb') as f:\n",
        "            pickle.dump(self.stellar_scaler, f)\n",
        "\n",
        "        # Scale all features\n",
        "        feature_names = features_cleaned.columns.tolist()\n",
        "        features_scaled = self.scaler.fit_transform(features_cleaned)\n",
        "        with open(self.scaler_file, 'wb') as f:\n",
        "            pickle.dump(self.scaler, f)\n",
        "\n",
        "        features_df = pd.DataFrame(features_scaled, columns=feature_names, index=cleaned_index)\n",
        "        self.feature_columns = feature_names\n",
        "\n",
        "        datasets['ExoMiner_Features'] = features_df\n",
        "        print(\"✅ Features engineered\")\n",
        "        print(f\"Original samples: {len(original_index)}, After cleaning: {len(cleaned_index)}\")\n",
        "        print(f\"Feature columns: {self.feature_columns}\")\n",
        "        return datasets, cleaned_index\n",
        "\n",
        "    def remove_outliers_iqr(self, df, threshold=2.0):\n",
        "        \"\"\"Remove outliers using Interquartile Range method\"\"\"\n",
        "        clean_df = df.copy()\n",
        "        for column in df.columns:\n",
        "            if df[column].dtype in ['float64', 'int64']:\n",
        "                Q1 = df[column].quantile(0.25)\n",
        "                Q3 = df[column].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - threshold * IQR\n",
        "                upper_bound = Q3 + threshold * IQR\n",
        "                mask = (clean_df[column] >= lower_bound) & (clean_df[column] <= upper_bound)\n",
        "                clean_df = clean_df[mask]\n",
        "        return clean_df\n",
        "\n",
        "## LIGHT CURVE PROCESSING\n",
        "class LightCurveProcessor:\n",
        "    \"\"\"Process light curves for AstroNet input with global/local views\"\"\"\n",
        "    def __init__(self, global_length=2001, local_length=201):\n",
        "        self.global_length = global_length\n",
        "        self.local_length = local_length\n",
        "\n",
        "    def download_single_lightcurve(self, tic_id):\n",
        "        \"\"\"Download TESS light curve for a given TIC ID (with caching)\"\"\"\n",
        "        cache_file = f\"lightcurve_{tic_id}.pkl\"\n",
        "        if os.path.exists(cache_file):\n",
        "            try:\n",
        "                return pd.read_pickle(cache_file)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Cache read error for TIC {tic_id}: {e}\")\n",
        "                os.remove(cache_file)\n",
        "\n",
        "        try:\n",
        "            search_result = lk.search_lightcurve(f\"TIC {tic_id}\", mission='TESS', author='SPOC')\n",
        "            if len(search_result) == 0:\n",
        "                print(f\"❌ No light curves found for TIC {tic_id}\")\n",
        "                return None\n",
        "\n",
        "            lc_collection = search_result.download_all()\n",
        "            if lc_collection is None or len(lc_collection) == 0:\n",
        "                print(f\"❌ No data downloaded for TIC {tic_id}\")\n",
        "                return None\n",
        "\n",
        "            lc = lc_collection.stitch().remove_nans().normalize()\n",
        "\n",
        "            time = np.array(lc.time.value, dtype=float)\n",
        "            flux = np.array(lc.flux.value, dtype=float)\n",
        "            flux_err = np.array(lc.flux_err.value, dtype=float) if lc.flux_err is not None else np.full_like(flux, np.nan)\n",
        "\n",
        "            lc_data = {'tic_id': tic_id, 'time': time, 'flux': flux, 'flux_err': flux_err}\n",
        "            pd.to_pickle(lc_data, cache_file)\n",
        "\n",
        "            print(f\"✅ Downloaded TIC {tic_id} with {len(time)} points\")\n",
        "            return lc_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Download error for TIC {tic_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def preprocess_lightcurve(self, time, flux, period=None, detrend=True, normalize=True, augment=False):\n",
        "        \"\"\"Preprocess single light curve\"\"\"\n",
        "        mask = np.isfinite(time) & np.isfinite(flux)\n",
        "        time_clean, flux_clean = time[mask], flux[mask]\n",
        "        if len(flux_clean) < 100:\n",
        "            return None, None\n",
        "\n",
        "        if normalize:\n",
        "            flux_clean = flux_clean / np.median(flux_clean)\n",
        "\n",
        "        if detrend and len(flux_clean) > 101:\n",
        "            try:\n",
        "                window_length = min(101, len(flux_clean) - 1)\n",
        "                if window_length % 2 == 0:\n",
        "                    window_length -= 1\n",
        "                trend = savgol_filter(flux_clean, window_length, 2)\n",
        "                flux_clean /= trend\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ Detrending failed: {e}\")\n",
        "\n",
        "        if augment:\n",
        "            flux_clean += np.random.normal(0, 0.01 * np.std(flux_clean), len(flux_clean))\n",
        "\n",
        "        # Create global view (full light curve)\n",
        "        global_view = self.resample_to_fixed_length(flux_clean, self.global_length)\n",
        "\n",
        "        # Create local view (zoomed around center or mid transit)\n",
        "        mid_idx = len(flux_clean) // 2\n",
        "        half_window = self.local_length // 2\n",
        "        start = max(0, mid_idx - half_window)\n",
        "        end = min(len(flux_clean), mid_idx + half_window)\n",
        "        local_flux = flux_clean[start:end]\n",
        "        local_view = self.resample_to_fixed_length(local_flux, self.local_length)\n",
        "\n",
        "        return global_view, local_view\n",
        "\n",
        "    def resample_to_fixed_length(self, flux, length):\n",
        "        \"\"\"Resample flux to a fixed sequence length\"\"\"\n",
        "        if len(flux) == length:\n",
        "            return flux\n",
        "        x_old = np.linspace(0, 1, len(flux))\n",
        "        x_new = np.linspace(0, 1, length)\n",
        "        interp = interp1d(x_old, flux, kind='linear', fill_value='extrapolate')\n",
        "        return interp(x_new)\n",
        "def preprocess_lightcurve(self, time, flux, period=None, detrend=True, normalize=True, augment=False):\n",
        "    \"\"\"Preprocess single light curve\"\"\"\n",
        "    mask = np.isfinite(time) & np.isfinite(flux)\n",
        "    time_clean, flux_clean = time[mask], flux[mask]\n",
        "    if len(flux_clean) < 100:\n",
        "        return None, None\n",
        "\n",
        "    if normalize:\n",
        "        flux_clean = flux_clean / np.median(flux_clean)\n",
        "\n",
        "    if detrend and len(flux_clean) > 101:\n",
        "        try:\n",
        "            window_length = min(101, len(flux_clean) - 1)\n",
        "            if window_length % 2 == 0:\n",
        "                window_length -= 1\n",
        "            trend = savgol_filter(flux_clean, window_length, 2)\n",
        "            flux_clean /= trend\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Detrending failed: {e}\")\n",
        "\n",
        "    if augment:\n",
        "        flux_clean += np.random.normal(0, 0.01 * np.std(flux_clean), len(flux_clean))\n",
        "\n",
        "    # Create global view (full light curve)\n",
        "    global_view = self.resample_to_fixed_length(flux_clean, self.global_length)\n",
        "\n",
        "    # Create local view (zoomed around center or mid transit)\n",
        "    mid_idx = len(flux_clean) // 2\n",
        "    half_window = self.local_length // 2\n",
        "    start = max(0, mid_idx - half_window)\n",
        "    end = min(len(flux_clean), mid_idx + half_window)\n",
        "    local_flux = flux_clean[start:end]\n",
        "    local_view = self.resample_to_fixed_length(local_flux, self.local_length)\n",
        "\n",
        "    return global_view, local_view\n",
        "    def resample_to_fixed_length(self, flux, length):\n",
        "        \"\"\"Resample flux to a fixed sequence length\"\"\"\n",
        "        if len(flux) == length:\n",
        "            return flux\n",
        "        x_old = np.linspace(0, 1, len(flux))\n",
        "        x_new = np.linspace(0, 1, length)\n",
        "        interp = interp1d(x_old, flux, kind='linear', fill_value='extrapolate')\n",
        "        return interp(x_new)\n",
        "\n",
        "#Transfer Learning\n",
        "class ExoplanetTransferLearningModel:\n",
        "    \"\"\"Improved AstroNet-like model\"\"\"\n",
        "    def __init__(self, global_shape=(2001, 1), local_shape=(201, 1), stellar_shape=(3,), num_classes=1):\n",
        "        self.global_shape = global_shape\n",
        "        self.local_shape = local_shape\n",
        "        self.stellar_shape = stellar_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = None\n",
        "\n",
        "    def create_model(self):\n",
        "        global_input = layers.Input(shape=self.global_shape, name='global_lc')\n",
        "        local_input = layers.Input(shape=self.local_shape, name='local_lc')\n",
        "        stellar_input = layers.Input(shape=self.stellar_shape, name='stellar_params')\n",
        "\n",
        "        def cnn_branch(input_layer, filters=[16, 32, 64, 128, 256]):\n",
        "            x = input_layer\n",
        "            for f in filters:\n",
        "                x = layers.Conv1D(f, 5, activation='relu', padding='same')(x)\n",
        "                x = layers.MaxPooling1D(5, strides=2, padding='same')(x)\n",
        "            x = layers.Flatten()(x)\n",
        "            return x\n",
        "\n",
        "        x_global = cnn_branch(global_input)\n",
        "        x_local = cnn_branch(local_input)\n",
        "\n",
        "        x = layers.Concatenate()([x_global, x_local])\n",
        "        x = layers.Dense(256, activation='relu')(x)\n",
        "        residual = layers.Dense(256, activation='relu')(x)\n",
        "        x = layers.Add()([x, residual])\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "\n",
        "        s = layers.Dense(32, activation='relu')(stellar_input)\n",
        "        x = layers.Concatenate()([x, s])\n",
        "        x = layers.Dense(128, activation='relu')(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "\n",
        "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "        self.model = keras.Model(inputs=[global_input, local_input, stellar_input], outputs=outputs)\n",
        "        return self.model\n",
        "\n",
        "    def compile_model(self, learning_rate=0.001):\n",
        "        self.model.compile(\n",
        "            optimizer=keras.optimizers.Adam(learning_rate),\n",
        "            loss=BinaryFocalCrossentropy(alpha=0.25, gamma=2.0),\n",
        "            metrics=['accuracy', keras.metrics.Precision(name='precision'),\n",
        "                     keras.metrics.Recall(name='recall'), keras.metrics.AUC(name='auc')]\n",
        "        )\n",
        "        print(\"✅ Model compiled with focal loss\")\n",
        "\n",
        "    def setup_callbacks(self):\n",
        "        return [\n",
        "            callbacks.EarlyStopping(monitor='val_auc', patience=20, mode='max', restore_best_weights=True),\n",
        "            callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=10, min_lr=1e-6, mode='max'),\n",
        "            callbacks.ModelCheckpoint('best_model.h5', monitor='val_auc', save_best_only=True, mode='max')\n",
        "        ]\n",
        "\n",
        "#Model Training and Evaluation\n",
        "class ModelTrainer:\n",
        "    \"\"\"Train and evaluate the exoplanet detection model\"\"\"\n",
        "    def __init__(self):\n",
        "        self.history = None\n",
        "\n",
        "    def prepare_lc_datasets(self, datasets, cleaned_index, processor, preprocessor):\n",
        "        global_lc = []\n",
        "        local_lc = []\n",
        "        stellar = []\n",
        "        labels = []\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "\n",
        "        for idx in cleaned_index:\n",
        "            tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "            period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "            label = exominer_data.loc[idx, 'label']\n",
        "            lc_data = processor.download_single_lightcurve(tic_id)\n",
        "            if lc_data is not None:\n",
        "                g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "                if g_flux is not None and l_flux is not None:\n",
        "                    global_lc.append(g_flux)\n",
        "                    local_lc.append(l_flux)\n",
        "                    stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]',\n",
        "                                                         'Stellar Radius [Solar Radii]',\n",
        "                                                         'Stellar Surface Gravity [log10(cm/s*2)]']].values\n",
        "                    stellar_row = np.nan_to_num(stellar_row)\n",
        "\n",
        "                    # Scale stellar features using the stellar scaler\n",
        "                    try:\n",
        "                        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "                            scaler = pickle.load(f)\n",
        "                        stellar_row = scaler.transform(stellar_row.reshape(1, -1)).flatten()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading stellar scaler for TIC {tic_id}: {e}. Using raw stellar features.\")\n",
        "                        stellar_row = stellar_row  # Fallback to raw features\n",
        "                    stellar.append(stellar_row)\n",
        "                    labels.append(label)\n",
        "\n",
        "        if not global_lc:\n",
        "            raise ValueError(\"No valid light curves processed.\")\n",
        "\n",
        "        global_lc = np.array(global_lc).reshape(-1, processor.global_length, 1)\n",
        "        local_lc = np.array(local_lc).reshape(-1, processor.local_length, 1)\n",
        "        stellar = np.array(stellar)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        X_train_g, X_test_g, X_train_l, X_test_l, X_train_s, X_test_s, y_train, y_test = train_test_split(\n",
        "            global_lc, local_lc, stellar, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "        )\n",
        "        X_train_g, X_val_g, X_train_l, X_val_l, X_train_s, X_val_s, y_train, y_val = train_test_split(\n",
        "            X_train_g, X_train_l, X_train_s, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        "        )\n",
        "\n",
        "        return X_train_g, X_val_g, X_test_g, X_train_l, X_val_l, X_test_l, X_train_s, X_val_s, X_test_s, y_train, y_val, y_test\n",
        "\n",
        "    def compute_class_weights(self, y_train):\n",
        "        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "        weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "        print(f\"📊 Class weights: {weight_dict}\")\n",
        "        return weight_dict\n",
        "\n",
        "    def train_model(self, model, X_train_g, X_train_l, X_train_s, y_train, X_val_g, X_val_l, X_val_s, y_val,\n",
        "                    epochs=200, batch_size=64, class_weight=None):\n",
        "        callbacks = model.setup_callbacks()\n",
        "        self.history = model.model.fit(\n",
        "            [X_train_g, X_train_l, X_train_s], y_train,\n",
        "            validation_data=([X_val_g, X_val_l, X_val_s], y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            class_weight=class_weight,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "        return self.history\n",
        "\n",
        "    def evaluate_model(self, model, X_test_g, X_test_l, X_test_s, y_test):\n",
        "        model.model = keras.models.load_model('best_model.h5')\n",
        "        y_pred_proba = model.model.predict([X_test_g, X_test_l, X_test_s], verbose=0)\n",
        "        y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "        test_accuracy = accuracy_score(y_test, y_pred)\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "        print(f\"\\n📈 TEST RESULTS:\")\n",
        "        print(f\" Accuracy: {test_accuracy:.4f}\")\n",
        "        print(f\" ROC AUC: {roc_auc:.4f}\")\n",
        "        print(f\" Precision: {report['1']['precision']:.4f}\")\n",
        "        print(f\" Recall: {report['1']['recall']:.4f}\")\n",
        "        return y_pred, y_pred_proba\n",
        "\n",
        "    def cross_validate(self, model_class, datasets, cleaned_index, processor, preprocessor, n_splits=5):\n",
        "        auc_scores = []\n",
        "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "        labels = datasets['ExoMiner']['label'].loc[cleaned_index]\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
        "            print(f\"Cross-validation Fold {fold+1}/{n_splits}\")\n",
        "            train_cleaned = cleaned_index[train_idx]\n",
        "            val_cleaned = cleaned_index[val_idx]\n",
        "            X_train_g, X_val_g, X_train_l, X_val_l, X_train_s, X_val_s, y_train, y_val = \\\n",
        "                self.prepare_lc_datasets_for_cv(datasets, train_cleaned, val_cleaned, processor, preprocessor)\n",
        "\n",
        "            model = model_class()\n",
        "            model.create_model()\n",
        "            model.compile_model()\n",
        "            self.train_model(model, X_train_g, X_train_l, X_train_s, y_train,\n",
        "                           X_val_g, X_val_l, X_val_s, y_val, epochs=50)\n",
        "            y_pred_proba = model.model.predict([X_val_g, X_val_l, X_val_s], verbose=0)\n",
        "            fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
        "            auc_scores.append(auc(fpr, tpr))\n",
        "\n",
        "        print(f\"\\nCross-validation AUC: {np.mean(auc_scores):.3f} ± {np.std(auc_scores):.3f}\")\n",
        "        return auc_scores\n",
        "\n",
        "    def prepare_lc_datasets_for_cv(self, datasets, train_index, val_index, processor, preprocessor):\n",
        "        global_lc_train = []\n",
        "        local_lc_train = []\n",
        "        stellar_train = []\n",
        "        labels_train = []\n",
        "        global_lc_val = []\n",
        "        local_lc_val = []\n",
        "        stellar_val = []\n",
        "        labels_val = []\n",
        "        exominer_data = datasets['ExoMiner']\n",
        "\n",
        "        # Process training data\n",
        "        for idx in train_index:\n",
        "            tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "            period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "            label = exominer_data.loc[idx, 'label']\n",
        "            lc_data = processor.download_single_lightcurve(tic_id)\n",
        "            if lc_data is not None:\n",
        "                g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "                if g_flux is not None and l_flux is not None:\n",
        "                    global_lc_train.append(g_flux)\n",
        "                    local_lc_train.append(l_flux)\n",
        "                    stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]',\n",
        "                                                         'Stellar Radius [Solar Radii]',\n",
        "                                                         'Stellar Surface Gravity [log10(cm/s*2)]']].values\n",
        "                    stellar_row = np.nan_to_num(stellar_row)\n",
        "                    try:\n",
        "                        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "                            scaler = pickle.load(f)\n",
        "                        stellar_row = scaler.transform(stellar_row.reshape(1, -1)).flatten()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading stellar scaler for TIC {tic_id}: {e}. Using raw stellar features.\")\n",
        "                        stellar_row = stellar_row\n",
        "                    stellar_train.append(stellar_row)\n",
        "                    labels_train.append(label)\n",
        "\n",
        "        # Process validation data\n",
        "        for idx in val_index:\n",
        "            tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "            period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "            label = exominer_data.loc[idx, 'label']\n",
        "            lc_data = processor.download_single_lightcurve(tic_id)\n",
        "            if lc_data is not None:\n",
        "                g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "                if g_flux is not None and l_flux is not None:\n",
        "                    global_lc_val.append(g_flux)\n",
        "                    local_lc_val.append(l_flux)\n",
        "                    stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]',\n",
        "                                                         'Stellar Radius [Solar Radii]',\n",
        "                                                         'Stellar Surface Gravity [log10(cm/s*2)]']].values\n",
        "                    stellar_row = np.nan_to_num(stellar_row)\n",
        "                    try:\n",
        "                        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "                            scaler = pickle.load(f)\n",
        "                        stellar_row = scaler.transform(stellar_row.reshape(1, -1)).flatten()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading stellar scaler for TIC {tic_id}: {e}. Using raw stellar features.\")\n",
        "                        stellar_row = stellar_row\n",
        "                    stellar_val.append(stellar_row)\n",
        "                    labels_val.append(label)\n",
        "\n",
        "        if not global_lc_train or not global_lc_val:\n",
        "            raise ValueError(\"No valid light curves processed for cross-validation.\")\n",
        "\n",
        "        X_train_g = np.array(global_lc_train).reshape(-1, processor.global_length, 1)\n",
        "        X_train_l = np.array(local_lc_train).reshape(-1, processor.local_length, 1)\n",
        "        X_train_s = np.array(stellar_train)\n",
        "        y_train = np.array(labels_train)\n",
        "        X_val_g = np.array(global_lc_val).reshape(-1, processor.global_length, 1)\n",
        "        X_val_l = np.array(local_lc_val).reshape(-1, processor.local_length, 1)\n",
        "        X_val_s = np.array(stellar_val)\n",
        "        y_val = np.array(labels_val)\n",
        "\n",
        "        return X_train_g, X_val_g, X_train_l, X_val_l, X_train_s, X_val_s, y_train, y_val\n",
        "\n",
        "#Hyper Parameter Tuning\n",
        "def tune_hyperparameters(X_train_g, X_train_l, X_train_s, y_train, X_val_g, X_val_l, X_val_s, y_val):\n",
        "    \"\"\"Tune hyperparameters using Keras Tuner\"\"\"\n",
        "    def build_model(hp):\n",
        "        model = ExoplanetTransferLearningModel()\n",
        "        model.create_model()\n",
        "        model.compile_model(learning_rate=hp.Float('lr', 1e-5, 1e-3, sampling='log'))\n",
        "        return model.model\n",
        "\n",
        "    tuner = kt.Hyperband(\n",
        "        build_model,\n",
        "        objective='val_auc',\n",
        "        max_epochs=200,\n",
        "        factor=3,\n",
        "        directory='tuner_dir',\n",
        "        project_name='exoplanet'\n",
        "    )\n",
        "\n",
        "    tuner.search([X_train_g, X_train_l, X_train_s], y_train,\n",
        "                 validation_data=([X_val_g, X_val_l, X_val_s], y_val),\n",
        "                 epochs=50)\n",
        "\n",
        "    best_hps = tuner.get_best_hyperparameters()[0]\n",
        "    print(f\"Best learning rate: {best_hps.get('lr')}\")\n",
        "    return tuner.get_best_models(1)[0], best_hps\n",
        "\n",
        "# ============================================\n",
        "# 7. VISUALIZATION AND ANALYSIS\n",
        "# ============================================\n",
        "def visualize_results(history, y_test, y_pred, y_pred_proba):\n",
        "    \"\"\"Visualize training history and evaluation metrics\"\"\"\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history.history['accuracy'], label='accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "    plt.plot(history.history['auc'], label='auc')\n",
        "    plt.plot(history.history['val_auc'], label='val_auc')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Metric Value')\n",
        "    plt.title('Training History')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Plot ROC curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=['Not Exoplanet', 'Exoplanet'],\n",
        "                yticklabels=['Not Exoplanet', 'Exoplanet'])\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "def test_with_real_lightcurves(model, datasets, preprocessor, num_samples=10):\n",
        "    \"\"\"Test the model with real light curves not used in training\"\"\"\n",
        "    print(f\"\\n🔬 Testing with {num_samples} real light curves:\")\n",
        "    exominer_data = datasets['ExoMiner']\n",
        "    all_indices = exominer_data.index\n",
        "    cleaned_index = datasets['ExoMiner_Features'].index\n",
        "    unseen_indices = all_indices.difference(cleaned_index)\n",
        "\n",
        "    if len(unseen_indices) < num_samples:\n",
        "        print(\"Not enough unseen samples to test.\")\n",
        "        return\n",
        "\n",
        "    test_indices = np.random.choice(unseen_indices, num_samples, replace=False)\n",
        "    processor = LightCurveProcessor()\n",
        "\n",
        "    try:\n",
        "        with open(preprocessor.stellar_scaler_file, 'rb') as f:\n",
        "            scaler = pickle.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading stellar scaler: {e}. Using raw stellar features.\")\n",
        "        scaler = None\n",
        "\n",
        "    for idx in test_indices:\n",
        "        tic_id = exominer_data.loc[idx, 'TIC ID']\n",
        "        period = exominer_data.loc[idx, 'Orbital Period [day]']\n",
        "        true_label = exominer_data.loc[idx, 'label']\n",
        "        lc_data = processor.download_single_lightcurve(tic_id)\n",
        "        if lc_data is not None:\n",
        "            g_flux, l_flux = processor.preprocess_lightcurve(lc_data['time'], lc_data['flux'], period=period)\n",
        "            if g_flux is not None and l_flux is not None:\n",
        "                stellar_row = exominer_data.loc[idx, ['Stellar Effective Temperature [K]',\n",
        "                                                    'Stellar Radius [Solar Radii]',\n",
        "                                                    'Stellar Surface Gravity [log10(cm/s*2)]']].values\n",
        "                stellar_row = np.nan_to_num(stellar_row).reshape(1, -1)\n",
        "                if scaler is not None:\n",
        "                    stellar_row = scaler.transform(stellar_row).flatten()\n",
        "                g_flux = np.array(g_flux).reshape(1, processor.global_length, 1)\n",
        "                l_flux = np.array(l_flux).reshape(1, processor.local_length, 1)\n",
        "                y_pred_proba = model.model.predict([g_flux, l_flux, stellar_row], verbose=0)\n",
        "                predicted_label = (y_pred_proba > 0.5).astype(int).flatten()[0]\n",
        "                print(f\" TIC ID: {tic_id}, True Label: {true_label}, \"\n",
        "                      f\"Predicted Probability: {y_pred_proba[0][0]:.4f}, Predicted Label: {predicted_label}\")\n",
        "            else:\n",
        "                print(f\" TIC ID: {tic_id} - Preprocessing failed.\")\n",
        "        else:\n",
        "            print(f\" TIC ID: {tic_id} - Light curve download failed.\")\n",
        "\n",
        "def analyze_feature_importance(datasets):\n",
        "    \"\"\"Analyze feature importance using RandomForest\"\"\"\n",
        "    print(\"\\n📊 Analyzing Feature Importance:\")\n",
        "    exominer_features = datasets['ExoMiner_Features']\n",
        "    labels = datasets['ExoMiner'].loc[exominer_features.index, 'label']\n",
        "\n",
        "    if len(exominer_features) == 0 or len(labels) == 0:\n",
        "        print(\"Not enough data to analyze feature importance.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            exominer_features, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "        )\n",
        "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        rf_model.fit(X_train, y_train)\n",
        "        importances = rf_model.feature_importances_\n",
        "        feature_names = exominer_features.columns\n",
        "        feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
        "        feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
        "        print(\"Top 10 Feature Importance:\")\n",
        "        print(feature_importance_df.head(10))\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x='importance', y='feature', data=feature_importance_df.head(10), palette='viridis')\n",
        "        plt.title('Top 10 Feature Importance from RandomForest')\n",
        "        plt.xlabel('Importance')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during feature importance analysis: {e}\")\n",
        "\n",
        "#Execution of pipeline\n",
        "def main():\n",
        "    \"\"\"Execute the exoplanet detection pipeline\"\"\"\n",
        "    datasets = load_and_parse_datasets()\n",
        "    preprocessor = ExoplanetDataPreprocessor()\n",
        "    datasets = preprocessor.create_labels(datasets)\n",
        "    datasets, cleaned_index = preprocessor.engineer_features(datasets)\n",
        "    processor = LightCurveProcessor()\n",
        "    trainer = ModelTrainer()\n",
        "\n",
        "    X_train_g, X_val_g, X_test_g, X_train_l, X_val_l, X_test_l, X_train_s, X_val_s, X_test_s, y_train, y_val, y_test = \\\n",
        "        trainer.prepare_lc_datasets(datasets, cleaned_index, processor, preprocessor)\n",
        "\n",
        "    best_model_hp, best_hps = tune_hyperparameters(\n",
        "        X_train_g, X_train_l, X_train_s, y_train, X_val_g, X_val_l, X_val_s, y_val\n",
        "    )\n",
        "\n",
        "    # Re-create and train the final model\n",
        "    model = ExoplanetTransferLearningModel()\n",
        "    model.create_model()\n",
        "    model.compile_model(learning_rate=best_hps.get('lr'))\n",
        "\n",
        "    X_train_g_full = np.concatenate((X_train_g, X_val_g))\n",
        "    X_train_l_full = np.concatenate((X_train_l, X_val_l))\n",
        "    X_train_s_full = np.concatenate((X_train_s, X_val_s))\n",
        "    y_train_full = np.concatenate((y_train, y_val))\n",
        "\n",
        "    class_weights = trainer.compute_class_weights(y_train_full)\n",
        "    history = trainer.train_model(\n",
        "        model, X_train_g_full, X_train_l_full, X_train_s_full, y_train_full,\n",
        "        X_test_g, X_test_l, X_test_s, y_test, epochs=200, batch_size=64, class_weight=class_weights\n",
        "    )\n",
        "\n",
        "    y_pred, y_pred_proba = trainer.evaluate_model(model, X_test_g, X_test_l, X_test_s, y_test)\n",
        "\n",
        "    visualize_results(history, y_test, y_pred, y_pred_proba)\n",
        "    test_with_real_lightcurves(model, datasets, preprocessor, num_samples=10)\n",
        "    analyze_feature_importance(datasets)\n",
        "    auc_scores = trainer.cross_validate(ExoplanetTransferLearningModel, datasets, cleaned_index, processor, preprocessor)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        model, history = main()\n",
        "        print(\"\\n🎉 PIPELINE COMPLETED!\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfVTOzwLXWNE",
        "outputId": "71851f85-90d2-4ca0-a9d9-2dca67e3f5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lightkurve\n",
            "  Downloading lightkurve-2.5.1-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: astropy>=5.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (7.1.0)\n",
            "Collecting astroquery>=0.3.10 (from lightkurve)\n",
            "  Downloading astroquery-0.4.11-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (4.13.5)\n",
            "Requirement already satisfied: bokeh>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (3.7.3)\n",
            "Collecting fbpca>=1.0 (from lightkurve)\n",
            "  Downloading fbpca-1.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.1 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (3.10.0)\n",
            "Collecting memoization>=0.3.1 (from lightkurve)\n",
            "  Downloading memoization-0.4.0.tar.gz (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.3.6 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (1.0.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (2.32.4)\n",
            "Collecting s3fs>=2024.6.1 (from lightkurve)\n",
            "  Downloading s3fs-2025.9.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (1.16.2)\n",
            "Requirement already satisfied: tqdm>=4.25.0 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (4.67.1)\n",
            "Collecting uncertainties>=3.1.4 (from lightkurve)\n",
            "  Downloading uncertainties-3.2.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: urllib3>=1.23 in /usr/local/lib/python3.12/dist-packages (from lightkurve) (2.5.0)\n",
            "Requirement already satisfied: pyerfa>=2.0.1.1 in /usr/local/lib/python3.12/dist-packages (from astropy>=5.0->lightkurve) (2.0.1.5)\n",
            "Requirement already satisfied: astropy-iers-data>=0.2025.4.28.0.37.27 in /usr/local/lib/python3.12/dist-packages (from astropy>=5.0->lightkurve) (0.2025.9.29.0.35.48)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from astropy>=5.0->lightkurve) (6.0.3)\n",
            "Requirement already satisfied: packaging>=22.0.0 in /usr/local/lib/python3.12/dist-packages (from astropy>=5.0->lightkurve) (25.0)\n",
            "Requirement already satisfied: html5lib>=0.999 in /usr/local/lib/python3.12/dist-packages (from astroquery>=0.3.10->lightkurve) (1.1)\n",
            "Requirement already satisfied: keyring>=15.0 in /usr/local/lib/python3.12/dist-packages (from astroquery>=0.3.10->lightkurve) (25.6.0)\n",
            "Collecting pyvo>=1.5 (from astroquery>=0.3.10->lightkurve)\n",
            "  Downloading pyvo-1.7-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.6.0->lightkurve) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.6.0->lightkurve) (4.15.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (3.1.6)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (1.3.3)\n",
            "Requirement already satisfied: narwhals>=1.13 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (2.6.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (11.3.0)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.12/dist-packages (from bokeh>=2.3.2->lightkurve) (2025.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.1->lightkurve) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.6->lightkurve) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.6->lightkurve) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->lightkurve) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->lightkurve) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->lightkurve) (2025.8.3)\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading aiobotocore-2.24.2-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting fsspec==2025.9.0 (from s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from s3fs>=2024.6.1->lightkurve) (3.12.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->lightkurve) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.24.0->lightkurve) (3.6.0)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting botocore<1.40.19,>=1.40.15 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading botocore-1.40.18-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve) (6.6.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2024.6.1->lightkurve) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (1.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2024.6.1->lightkurve) (1.20.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.12/dist-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from html5lib>=0.999->astroquery>=0.3.10->lightkurve) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=2.9->bokeh>=2.3.2->lightkurve) (3.0.3)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (3.4.0)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (0.9.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (3.4.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (4.3.0)\n",
            "Requirement already satisfied: jaraco.context in /usr/local/lib/python3.12/dist-packages (from keyring>=15.0->astroquery>=0.3.10->lightkurve) (6.0.1)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.12/dist-packages (from SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (43.0.3)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from jaraco.classes->keyring>=15.0->astroquery>=0.3.10->lightkurve) (10.8.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring>=15.0->astroquery>=0.3.10->lightkurve) (2.23)\n",
            "Downloading lightkurve-2.5.1-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astroquery-0.4.11-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m130.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3fs-2025.9.0-py3-none-any.whl (30 kB)\n",
            "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uncertainties-3.2.3-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiobotocore-2.24.2-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyvo-1.7-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading botocore-1.40.18-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: fbpca, memoization\n",
            "  Building wheel for fbpca (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fbpca: filename=fbpca-1.0-py3-none-any.whl size=11373 sha256=d03888548da6ad2f7e101d2890564500a96c093d9ad2cfd5f9c1cfdf0ccfa36e\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/15/cd/2f622795b09e83471a3be5d2581cd9cf96a6ec7aa78e8deffe\n",
            "  Building wheel for memoization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memoization: filename=memoization-0.4.0-py3-none-any.whl size=50452 sha256=611a8e0286cdaadff681be1e8ecd0f6d0e2f016ee3f0bed424feb165990d36b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/35/02/90618fc7cbf03a335f3cacd59d32b35930bf5a57f3c0d0814c\n",
            "Successfully built fbpca memoization\n",
            "Installing collected packages: fbpca, uncertainties, memoization, jmespath, fsspec, aioitertools, botocore, pyvo, aiobotocore, s3fs, astroquery, lightkurve\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiobotocore-2.24.2 aioitertools-0.12.0 astroquery-0.4.11 botocore-1.40.18 fbpca-1.0 fsspec-2025.9.0 jmespath-1.0.1 lightkurve-2.5.1 memoization-0.4.0 pyvo-1.7 s3fs-2025.9.0 uncertainties-3.2.3\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Collecting keras_tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (from keras_tuner) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras_tuner) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from keras_tuner) (2.32.4)\n",
            "Collecting kt-legacy (from keras_tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras->keras_tuner) (0.5.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->keras_tuner) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras->keras_tuner) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras_tuner) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras_tuner) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras_tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras_tuner\n",
            "Successfully installed keras_tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/lightkurve/prf/__init__.py:7: UserWarning: Warning: the tpfmodel submodule is not available without oktopus installed, which requires a current version of autograd. See #1452 for details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Libraries imported successfully\n",
            "TensorFlow version: 2.19.0\n",
            "Lightkurve version: 2.5.1\n",
            "Loading K2 dataset...\n",
            "Loading ExoMiner dataset...\n",
            "Loading TOI dataset...\n",
            "Merging ExoMiner with TOI...\n",
            "Loading Cumulative dataset...\n",
            "✅ Labels created\n",
            "ExoMiner - Planets: 7126, Non-planets: 5641\n",
            "✅ Features engineered\n",
            "Original samples: 12767, After cleaning: 8226\n",
            "Feature columns: ['Orbital Period [day]', 'Transit Duration [hour]', 'Transit Depth [ppm]', 'Planet Radius [Earth Radii]', 'MES', 'Transit Model SNR', 'Number of transits observed', 'Stellar Effective Temperature [K]', 'Stellar Radius [Solar Radii]', 'Stellar Surface Gravity [log10(cm/s*2)]', 'depth_duration_ratio', 'period_snr_ratio']\n",
            "✅ Downloaded TIC 239332587 with 690692 points\n",
            "✅ Downloaded TIC 24695044 with 376102 points\n",
            "✅ Downloaded TIC 416195870 with 1464070 points\n",
            "✅ Downloaded TIC 366115856 with 84588 points\n",
            "✅ Downloaded TIC 232612416 with 918935 points\n",
            "✅ Downloaded TIC 406941612 with 105284 points\n",
            "✅ Downloaded TIC 300604770 with 267353 points\n",
            "✅ Downloaded TIC 153078576 with 33495 points\n",
            "✅ Downloaded TIC 153951307 with 307043 points\n",
            "✅ Downloaded TIC 158002130 with 245917 points\n",
            "✅ Downloaded TIC 220475245 with 470426 points\n",
            "✅ Downloaded TIC 359271092 with 645666 points\n",
            "✅ Downloaded TIC 229605891 with 413015 points\n",
            "✅ Downloaded TIC 441763252 with 384656 points\n",
            "✅ Downloaded TIC 199444169 with 152581 points\n",
            "✅ Downloaded TIC 233823679 with 129127 points\n",
            "✅ Downloaded TIC 233087860 with 1824953 points\n",
            "✅ Downloaded TIC 160390955 with 66914 points\n",
            "✅ Downloaded TIC 101696403 with 12827 points\n",
            "✅ Downloaded TIC 219857012 with 3170779 points\n",
            "✅ Downloaded TIC 260304800 with 635018 points\n",
            "✅ Downloaded TIC 123357034 with 130218 points\n",
            "✅ Downloaded TIC 230017324 with 1159114 points\n",
            "✅ Downloaded TIC 232540264 with 1374061 points\n",
            "✅ Downloaded TIC 260417932 with 422356 points\n",
            "✅ Downloaded TIC 309791156 with 744444 points\n",
            "✅ Downloaded TIC 307956397 with 487824 points\n",
            "✅ Downloaded TIC 165723070 with 62387 points\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n",
            "WARNING:lightkurve.utils:Warning: 30% (5871/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Downloaded TIC 382626661 with 2016809 points\n",
            "✅ Downloaded TIC 54962195 with 62294 points\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: 30% (5881/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n",
            "WARNING:lightkurve.utils:Warning: 30% (5881/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Downloaded TIC 167415965 with 3250768 points\n",
            "✅ Downloaded TIC 186470968 with 51913 points\n",
            "✅ Downloaded TIC 58542531 with 259271 points\n",
            "✅ Downloaded TIC 130924120 with 47388 points\n",
            "✅ Downloaded TIC 272672133 with 517178 points\n",
            "✅ Downloaded TIC 284441182 with 160904 points\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: 30% (5874/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n",
            "WARNING:lightkurve.utils:Warning: 30% (5874/19412) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Downloaded TIC 278683844 with 3577891 points\n",
            "✅ Downloaded TIC 415969908 with 272632 points\n",
            "✅ Downloaded TIC 71431780 with 220172 points\n",
            "✅ Downloaded TIC 367858035 with 175219 points\n",
            "✅ Downloaded TIC 219508169 with 258533 points\n",
            "✅ Downloaded TIC 237086564 with 197458 points\n",
            "✅ Downloaded TIC 156007004 with 45486 points\n",
            "✅ Downloaded TIC 327369524 with 90758 points\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: 24% (4655/19050) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n",
            "WARNING:lightkurve.utils:Warning: 24% (4655/19050) of the cadences will be ignored due to the quality mask (quality_bitmask=17087).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Downloaded TIC 141205978 with 48123 points\n",
            "✅ Downloaded TIC 160045097 with 597066 points\n",
            "✅ Downloaded TIC 302381397 with 93788 points\n",
            "✅ Downloaded TIC 198390247 with 1758881 points\n",
            "✅ Downloaded TIC 75878355 with 456094 points\n",
            "✅ Downloaded TIC 462615350 with 474966 points\n",
            "✅ Downloaded TIC 142387023 with 304761 points\n",
            "✅ Downloaded TIC 184733148 with 30356 points\n",
            "✅ Downloaded TIC 406672232 with 298095 points\n",
            "✅ Downloaded TIC 237099296 with 611240 points\n",
            "✅ Downloaded TIC 161003569 with 50486 points\n",
            "✅ Downloaded TIC 270342589 with 121358 points\n",
            "✅ Downloaded TIC 280803917 with 131494 points\n",
            "✅ Downloaded TIC 141768070 with 722280 points\n",
            "✅ Downloaded TIC 77253676 with 66619 points\n",
            "✅ Downloaded TIC 147890655 with 41343 points\n",
            "✅ Downloaded TIC 257241363 with 772755 points\n",
            "✅ Downloaded TIC 239134248 with 60319 points\n",
            "✅ Downloaded TIC 272758199 with 353700 points\n",
            "✅ Downloaded TIC 376981340 with 41595 points\n",
            "✅ Downloaded TIC 272785423 with 355977 points\n",
            "✅ Downloaded TIC 233188747 with 481944 points\n",
            "✅ Downloaded TIC 467971286 with 134134 points\n",
            "✅ Downloaded TIC 470987100 with 46943 points\n",
            "✅ Downloaded TIC 268334473 with 645065 points\n",
            "✅ Downloaded TIC 318022259 with 145343 points\n",
            "✅ Downloaded TIC 452964680 with 63664 points\n",
            "✅ Downloaded TIC 137420801 with 86942 points\n",
            "✅ Downloaded TIC 417129824 with 49675 points\n",
            "✅ Downloaded TIC 287591638 with 53887 points\n",
            "✅ Downloaded TIC 317597583 with 727773 points\n",
            "✅ Downloaded TIC 365938305 with 411045 points\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eNH-3K_FXWv0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}